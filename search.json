[{"title":"K8s常用命令总结","path":"/2024/08/22/K8s常用命令总结/","content":"总结一下这次在项目中经常使用的一些 K8s 的命令。 K8s 常用命令总结 1. 获取 Kubernetes 集群信息1kubectl cluster-info 场景: 查看 Kubernetes 集群的基本信息，包括 API 服务器的地址。 2. 获取节点信息1kubectl get nodes 场景: 查看集群中所有节点的状态，帮助确认节点是否健康，以及节点资源的使用情况。 3. 获取 Pod 列表1kubectl get pods 场景: 查看指定命名空间中的所有 Pod 的状态，检查 Pod 是否正在运行。 4. 获取 Pod 的详细信息1kubectl describe pod &lt;pod-name&gt; -n &lt;namespace&gt; 场景: 获取 Pod 的详细描述，包括状态、事件、日志等，帮助进行故障排查。 5. 获取 Pod 的标签1kubectl get pod &lt;pod-name&gt; -o=jsonpath=&#x27;&#123;.metadata.labels&#125;&#x27; 场景: 获取某个 Pod 的标签，方便进行标签筛选和调度管理。 6. 删除 Pod1kubectl delete pod &lt;pod-name&gt; -n &lt;namespace&gt; 场景: 删除指定的 Pod。常用于清理挂掉或无用的 Pod。 7. 强制删除 Pod1kubectl delete pod &lt;pod-name&gt; --grace-period=0 --force -n &lt;namespace&gt; 场景: 强制删除 Pod，适用于无法正常删除的 Pod。注意，这种操作会跳过 Pod 的优雅终止过程。 8. 获取 Pod 的日志1kubectl logs &lt;pod-name&gt; -n &lt;namespace&gt; 场景: 查看 Pod 中容器的日志，帮助排查应用问题或容器启动失败的原因。 9. 查看 Pod 的实时日志（带容器名）1kubectl logs -f &lt;pod-name&gt; -c &lt;container-name&gt; -n &lt;namespace&gt; 场景: 实时查看指定容器的日志，适用于跟踪应用程序运行中的问题。 10. 进入 Pod 内部1kubectl exec -it &lt;pod-name&gt; -n &lt;namespace&gt; -- /bin/bash 场景: 进入 Pod 内部，执行交互式命令，适用于调试和手动操作。 11. 执行 Pod 内部命令1kubectl exec &lt;pod-name&gt; -n &lt;namespace&gt; -- &lt;command&gt; 场景: 在 Pod 内部执行一次性命令，适用于临时操作。 12. 查看命名空间中的所有资源1kubectl get all -n &lt;namespace&gt; 场景: 查看命名空间中的所有资源（Pod、Service、Deployment 等），用于快速浏览资源状态。 13. 部署应用（使用 YAML 配置）1kubectl apply -f &lt;file.yaml&gt; 场景: 根据指定的 YAML 文件创建或更新资源，常用于部署应用、配置服务等。 14. 删除命名空间1kubectl delete namespace &lt;namespace&gt; 场景: 删除指定命名空间及其资源，适用于清理整个环境。 15. 查看服务的详细信息1kubectl describe svc &lt;service-name&gt; -n &lt;namespace&gt; 场景: 查看服务的详细信息，包括端口、选择器等，适用于调试服务访问问题。 16. 获取集群的 ConfigMap 列表1kubectl get configmap -n &lt;namespace&gt; 场景: 查看命名空间中所有的 ConfigMap，帮助管理配置文件。 17. 创建 ConfigMap1kubectl create configmap &lt;configmap-name&gt; --from-file=&lt;path-to-file&gt; -n &lt;namespace&gt; 场景: 创建 ConfigMap 资源并从文件中导入配置，常用于存储配置文件或密钥。 18. 获取所有 Pod 的标签1kubectl get pods --show-labels -n &lt;namespace&gt; 场景: 获取所有 Pod 的标签信息，便于筛选和管理。 19. 获取某个资源的版本1kubectl version 场景: 查看 kubectl 客户端和 Kubernetes 服务端的版本信息，帮助确认版本兼容性。 20. 使用 Port Forward 转发端口1kubectl port-forward pod/&lt;pod-name&gt; 8080:80 -n &lt;namespace&gt; 场景: 将 Pod 的端口转发到本地，适用于访问集群内的应用（如本地访问 Pod 中的 Web 服务）。 21. 查看 Pod 的状态1kubectl get pod &lt;pod-name&gt; -n &lt;namespace&gt; -o wide 场景: 查看 Pod 的状态和其他详细信息，如 IP 地址、节点等，帮助诊断 Pod 问题。 22. 查看 Deployment 状态1kubectl get deployment &lt;deployment-name&gt; -n &lt;namespace&gt; 场景: 查看某个 Deployment 的状态，帮助确认应用是否按预期运行。 23. 获取 Kubernetes 的 Config 配置文件1kubectl config view 场景: 查看当前的 Kubernetes 配置，帮助查看已配置的集群和上下文。 24. 修改 kubectl 使用的 Config 文件1export KUBECONFIG=&lt;path-to-kubeconfig&gt; 场景: 指定自定义的 kubeconfig 文件位置，避免使用默认的 ~/.kube/config。 25. 等待 Pod 删除完成1kubectl wait --for=delete pod &lt;pod-name&gt; -n &lt;namespace&gt; --timeout=60s 场景: 等待指定的 Pod 删除完成，用于确保 Pod 被完全删除。 26. 查看 Pod 的进程1kubectl top pod &lt;pod-name&gt; -n &lt;namespace&gt; 场景: 查看 Pod 中的资源使用情况（CPU、内存等），适用于性能调优和资源监控。 27. 查看命名空间的资源1kubectl get all -n &lt;namespace&gt; 场景: 查看某个命名空间中的所有资源，帮助管理和维护集群。 28. 监控服务的状态1kubectl rollout status deployment/&lt;deployment-name&gt; -n &lt;namespace&gt; 场景: 查看 Deployment 的发布状态，适用于查看应用更新或滚动升级的进度。 29. 更新资源1kubectl apply -f &lt;file.yaml&gt; 场景: 更新现有的 Kubernetes 资源，适用于应用更新或配置变更。 30. 获取 Helm 安装的 Release 列表1helm list -n &lt;namespace&gt; 场景: 获取 Helm 安装的所有 Release，帮助管理 Helm 部署的应用。","tags":["K8s"]},{"title":"自动化混沌测试框架的设计与实现","path":"/2024/08/22/自动化混沌测试框架的设计与实现/","content":"回顾总结一下OSPP2024中做的项目：搭建自动化 RocketMQ Chaos 测试基础设施，并提升 Apache RocketMQ 测试流水线质量 1. 技术选型1.1 OpenChaosOpenChaos 是openmessage组织下开源的一款混沌测试框架。其主要目标是通过模拟各种故障场景，验证消息系统在高并发和故障条件下的稳定性和可靠性。OpenChaos 提供了一些预定义的测试模型，能够检测系统在网络延迟、网络分区、节点崩溃等情况下的行为。 1.1.1 工作原理OpenChaos的架构图如下： 整体架构可以分为管理层、执行层与被测组件层。 管理层负责整个测试流程的控制，确保测试的各个步骤按预定顺序进行。 中间的执行层是OpenChaos的核心组件，Model定义了对分布式系统进行操作的基本形式，包括测试步骤和方法；Detection Model用来给系统注入故障模型；Metrics和Measurement Model负责监测被测集群的表现和根据测试结果生成可视化图表。 被测组件层指的是待测试的分布式系统，可以通过实现OpenChaos的API接入驱动。 OpenChaos 的工作原理是： 控制节点负责管理整个测试流程，将集群节点组成待测试的分布式集群，并根据需要测试的系统找到并加载对应的 Driver 组件，创建相应数量的客户端。 控制节点按照Model 组件定义的流程，指挥客户端对集群进行操作。 在测试过程中，Detection Model 会根据不同的观察需求在集群节点上引入故障或事件。 Metrics 模块则监测集群的表现，收集性能数据。 测试结束后，检查器组件自动分析业务和非业务数据，生成测试结果并以可视化图表的形式展示。 1.1.2 消息检测模型（QueueModel）OpenChaos提供了对分布式系统进行混沌测试的基础框架，可以有效的检测分布式集群在各种边缘故障环境下的韧性表现。它提供的消息检测模型可以很好地检测分布式消息系统的表现。完成本项目需要对OpenChaso提供的消息检测模型有足够深入的了解。通过阅读OpenChaos的源码可以初步认识该消息检测模型： 在chaos-framework/src/main/java/io/openchaos/model包下的 Model 接口定义了 OpenChaos 中检测模型的核心操作和方法，提供了管理测试集群和执行测试的基本功能。该接口的方法及其作用如下： 12345678910111213141516171819202122public interface Model extends MetaDataSupport &#123; // 设置所有客户端(client节点),包括初始化客户端实例并配置它们以便与测试集群进行交互 void setupClient(); // 设置要测试的集群。driverConfiguration : 驱动程序的配置，指定如何配置和管理集群节点 Map&lt;String, ChaosNode&gt; setupCluster(DriverConfiguration driverConfiguration, boolean isInstall, boolean restart); // 确保集群已准备就绪 boolean probeCluster(); // 开始测试，启动测试过程，包括故障注入和性能监控 void start(); // 停止测试 void stop(); // 测试停止后执行的操作,用于清理或收集测试后的数据 void afterStop(); // 关闭模型 void shutdown(); // 获取MetaNode,在RocketMQ指NameServer String getMetaNode(); // 获取可以区分集群的名称，在RocketMQ中指clusterName String getMetaName(); // 获取状态类的类名 String getStateName();&#125; 其中针对消息系统的检测模型是QueueModel，它是 Model 接口的一个实现类，用于测试消息队列系统的可靠性和稳定性。它具体实现了 Model 接口中定义的方法，并且添加了很多与消息队列相关的特定功能。 其构造函数和一些重要的成员变量如下： 123456789101112131415public QueueModel(int concurrency, RateLimiter rateLimiter, Recorder recorder, File driverConfigFile, boolean isOrderTest, boolean isUsePull, List&lt;String&gt; shardingKeys) &#123; this.concurrency = concurrency; // 并发数，决定同时运行的客户端数量 this.recorder = recorder; // 记录器，记录请求/响应的数据 this.driverConfigFile = driverConfigFile; // 驱动的配置文件 this.rateLimiter = rateLimiter; clients = new ArrayList&lt;&gt;(); // 存储客户端线程的列表 workers = new ArrayList&lt;&gt;(); // 工作线程列表 cluster = new HashMap&lt;&gt;(); metaNodesMap = new HashMap&lt;&gt;(); chaosTopic = String.format(&quot;%s-chaos-topic&quot;, DATE_FORMAT.format(new Date())); this.isOrderTest = isOrderTest; // 是否进行有序测试 this.isUsePull = isUsePull; // 是否使用拉取模式 this.shardingKeys = shardingKeys;&#125; QueueModel在设置完集群setupCluster()和客户端setupClient()之后，通过start()启动： 对于消息系统，会启动所有的QueueClient的工作线程，负责消息的生产和消费操作。它的nextInvoke()方法负责向群集发出请求,根据生成的操作进行相应的生产或消费操作，并记录请求和响应日志。 12345678910111213141516171819202122public void nextInvoke() &#123; // ... if (op.getInvokeOperation().equals(&quot;enqueue&quot;)) &#123; // 生产操作 InvokeResult invokeResult; if (isOrderTest) &#123; // 顺序测试时，选择分片键并记录请求和响应日志 String shardingKey = shardingKeys.get(random.nextInt(shardingKeys.size())); requestLogEntry.shardingKey = shardingKey; recorder.recordRequest(requestLogEntry); invokeResult = producer.enqueue(shardingKey, op.getValue().getBytes()); recorder.recordResponse(new ResponseLogEntry(clientId, op.getInvokeOperation(), invokeResult, shardingKey, op.getValue(), System.currentTimeMillis(), System.currentTimeMillis() - requestLogEntry.timestamp, invokeResult.getExtraInfo())); &#125; else &#123; // 非顺序测试时，直接记录请求和响应日志 // ... &#125; &#125; else &#123; // ... // 消费操作，调用 dequeue 方法，记录响应日志 &#125;&#125; stop()停止测试，并在测试结束后执行客户端的最后一次调用： 123public void afterStop() &#123; clients.forEach(Client::lastInvoke); // 如果Queue在usePull模式下，lastInvoke会通过循环拉取和记录(drain)消息，确保所有剩余消息都得到处理 &#125; 最后，关闭客户端和集群节点。 以下是 QueueModel 进行消息系统测试的流程图： OpenChaos提供的消息检测模型可以在各种故障场景下有效地检测消息系统的关键指标： 消息丢失检测：确保在各种故障场景下，消息不会丢失。 可用性检测：确保系统在故障发生时仍然具有预期的可用性。 消息顺序性验证：检测消息系统在故障场景下是否能维持消息顺序的正确性。 这一模型在确保消息系统的可靠性和稳定性方面具有很高的借鉴价值。在本项目中需要对OpenChaos的消息检测模型有比较深刻的认识之后，再利用这一模型结合Chaos Mesh等混沌测试平台对RocketMQ集群进行测试。 1.2 Chaos Mesh1.2.1 原理简介Chaos Mesh是一个基于Kubernetes的云原生混沌工程平台，内置了丰富的故障模拟类型，可以很方便地在开发测试中以及生产环境中模拟系统在现实世界中可能出现的各类异常，及时发现系统潜在的问题。 Chaos Mesh基于 Kubernetes CRD (Custom Resource Definition) 构建。其工作原理可以简要概述为： 故障类型对应Kubernetes的CRD：将不同的故障类型定义为多个CRD类型。这些CRD类型包括网络故障、节点故障、磁盘故障等。用户可以通过创建相应的CRD对象来描述和配置具体的故障模拟实验。 每种故障类型由单独的Controller控制：每种故障类型实现了单独的Controller，这些Controller负责监听和处理对应CRD对象的变更事件，根据用户的配置启动、停止或调整相应的混沌实验。 执行混沌实验：当用户创建或更新CRD对象时，对应的Controller会根据CRD对象中的配置信息启动相应的混沌实验，在Kubernetes集群中模拟各种故障和异常条件。 测试结果分析：在混沌实验执行完成后，Chaos Mesh根据测试的数据评估系统在面对各种异常情况时的表现。 1.2.2 定义混沌实验并进行故障注入通过对Chaos Mesh原理而简要分析之后，可知使用Chaos Mesh进行混沌测试的关键在于如何定义混沌实验的类型并将故障注入到待测试的系统中。在学习了Chaos Mesh的文档和运行demo示例之后，我对Chaos Mesh的使用有了初步的认识： 定义混沌实验： 在Chaos Mesh中可以通过编写对应 YAML 文件或者在Dashboard上操作这两种方式定义混沌实验，很明显编写YAML的方式更符合本项目的需求。在定义混沌实验时，可以通过selector指定实验作用的范围，还可以通过创建不同的Schedule对象来定义实验的调度规则，比如可以使用cron表达式作为schedule字段的值来执行定时任务，从而在固定的时间（或根据固定的时间间隔）自动新建混沌实验。 下面是一个混沌实验（名为network-delay.yaml的文件）的定义示例： 12345678910111213141516171819202122apiVersion: chaos-mesh.org/v1alpha1kind: Schedulemetadata: name: schedule-delay-examplespec: schedule: &#x27;5 * * * *&#x27; historyLimit: 2 concurrencyPolicy: &#x27;Allow&#x27; type: &#x27;NetworkChaos&#x27; networkChaos: action: delay mode: one selector: namespaces: - default labelSelectors: &#x27;app&#x27;: &#x27;web-show&#x27; delay: latency: &#x27;10ms&#x27; correlation: &#x27;100&#x27; jitter: &#x27;0ms&#x27; duration: &#x27;12s&#x27; 依据此配置，Chaos Mesh 将会在每个小时的第五分钟（比如 0:05, 1:05…）创建 NetworkChaos 对象。 运行实验，将故障注入系统： 定义完混沌实验的类型之后，仍以上面的network-delay.yaml举例，使用kubectl apply -f创建并运行该指令： 1kubectl apply -f network-delay.yaml 使用 kubectl describe 命令检查混沌实验的运行情况： 1kubectl describe networkchaos network-delay 结束混沌实验后，使用 kubectl delete 命令删除混沌实验： 1kubectl delete networkchaos network-delay 1.2.3 OpenChaos的消息检测模型与Chaos Mesh的结合经过前面的调研，可以认识到Chaos Mesh和OpenChaos的定位是不太相同的，Chaos Mesh主要提供比较完整的混沌实验故障注入的手段，而OpenChaos则是需要针对某些类型中间件做固定Module运行。而本项目是针对RocketMQ集群进行测试，利用OpenChaos提供的队列模型对消息系统的消息语义进行验证，并结合Chaos Mesh测试平台在Kubernetes环境下方便部署的特性来为RocketMQ构建自动化的测试框架是一套更为准确的方案。 但是按照OpenChaos的使用方法来搭建测试环境需要一些准备工作和手动配置，是比较繁琐的。为了在Kubernetes环境下更方便地部署和管理这个测试框架，可以考虑将OpenChaos配置和运行脚本容器化，部署到Kubernetes中，这样能够简化其部署流程，使其能够在Kubernetes集群中自动化地运行消息系统测试。结合Chaos Mesh，可以在同一环境中注入各种故障，模拟真实的生产环境，从而更加全面和准确地验证RocketMQ的稳定性和容错性。 可以用下图概括： 2. 搭建自动化 RocketMQ Chaos 测试基础设施通过对项目的调研，对于为RocketMQ搭建一个自动化Chaos测试框架有了一个初步的实现方案：在 Kubernetes 环境中搭建起RocketMQ集群，并将OpenChaos容器化并运行在同一个Kubernetes 环境中对 RocketMQ 集群进行消息语义验证，并结合 Chaos Mesh 进行故障注入，最后收集测试数据并分析结果。编写自动化脚本执行这一测试流程。最终将该框架集成到 GitHub CI 中，实现持续集成测试。 框架的总体设计包括以下几个部分： RocketMQ集群部署：在Kubernetes中部署并运行RocketMQ集群。 OpenChaos测试容器化：将OpenChaos的测试部分容器化，简化其部署和运行。 Chaos Mesh故障注入：利用Chaos Mesh注入故障，实现对RocketMQ集群的各种故障模拟。 Kubernetes编排：通过Kubernetes编排测试和故障注入，自动化执行测试流程。 以下是详细流程图，展示了框架的工作流程： 关键步骤： 环境准备： 安装Kubernetes集群，Chaos Mesh，准备RocketMQ集群的部署配置。 编辑OpenChaos的配置文件rocketmq.yaml，配置RocketMQ的相关信息。 使用Helm搭建起RocketMQ集群 ： https://github.com/apache/rocketmq-docker/tree/master/rocketmq-k8s-helm 创建Dockerfile，将OpenChaos的测试部分容器化，构建Docker镜像。 创建Kubernetes Job来运行OpenChaos测试容器。 创建Chaos Mesh CRD，进行故障注入。 执行测试： 确保RocketMQ集群已经运行。 部署OpenChaos测试Job，开始消息系统的测试。 在测试过程中，通过Chaos Mesh注入故障，监控RocketMQ的行为和OpenChaos的测试结果。 等待OpenChaos测试Job完成，生成测试结果文件。 分析测试结果，生成报告。 清理测试环境，释放资源。 最后可以集成到GitHub Actions中，实现自动化部署和测试流程。","tags":["OSPP2024"]},{"title":"为Rocketmq增加集成测试流水线","path":"/2024/08/22/为Rocketmq增加集成测试流水线/","content":"1、项目要求项目描述 Apache RocketMQ 当前有一些测试流水线，包括 单测、e2e 测试等，可以参考https://github.com/apache/rocketmq/tree/develop/.github/workflows这些流水线保障 Apache RocketMQ 的代码质量，但仍然存在一些优化的内容： 流水线运行不稳定，需要手动重试：一些单元测试不稳定，导致流水线运行失败，需要手动重试。针对上述问题，需要提升单测的稳定性，此外，如果运行失败，可以进行有限次数的重试，减少人为介入。 缺失集成测试流水线：RocketMQ 测试流水线会跑 RocketMQ 单测，但当前流水线未运行集成测试（RocketMQ test 模块），需要增加集成测试流水线，同时不影响流水线运行的整体效率（单测流水线和集成测试流水线可以并行运行）。 缺少性能基准测试流水线：一些代码的合入可能会影响性，需要性能测试流水线，在同一个性能基准下，衡量不同代码版本间的性能差距。性能的衡量指标包括 TPS 和 RT 等 2、前置知识2.1、单元测试与集成测试的区别2.2、Maven中与集成测试相关知识2.3、当前仓库中关于单元测试与集成测试的配置本地执行集成测试： 增加一个profile调过单元测试-Pskip-unit-tests 运行集成测试：mvn clean verify -Pit-test -Pskip-unit-tests 3、项目具体实现","tags":["OSPP2024"]},{"title":"计算机网络：Top-Down","path":"/2024/06/07/计算机网络/","content":"期末计算机网络知识复习总结 1. 计算机网络基本概念 Q：什么是计算机网络？ 计算机网络是由多个计算机系统及其他硬件设备通过通信链路互联而成的系统，旨在共享资源、数据和信息。网络中的计算机和设备通过协议（如TCP&#x2F;IP）进行通信，确保数据在不同设备之间可靠传输。计算机网络可以按规模、连接方式和功能进行分类，如局域网（LAN）、广域网（WAN）和互联网 基本组成部分 终端设备：计算机、服务器、智能手机、平板电脑等，用于发送和接收数据。 网络设备：路由器、交换机、集线器、网关等，用于数据传输和管理。 通信介质：有线（如以太网电缆、光纤）或无线（如Wi-Fi、蓝牙）介质，用于连接设备。 协议：一组规则和标准，如TCP&#x2F;IP、HTTP、FTP，用于确保设备间通信的正确性和有效性。 网络的类型 局域网（LAN）：覆盖小范围（如办公室、家庭）的网络，传输速率高，延迟低。常用的技术有以太网、Wi-Fi。 城域网（MAN）：覆盖一个城市或多个建筑的网络，规模和传输距离介于LAN和WAN之间。 广域网（WAN）：覆盖大范围（如国家、洲际）的网络，传输距离远，速率相对较低。互联网就是最大的WAN。 个人局域网（PAN）：覆盖个人使用范围的网络，如蓝牙、红外连接 计算机网络的应用 互联网：全球最大的计算机网络，提供信息检索、电子邮件、电子商务、社交网络等服务。 企业网络：支持企业内部通信和资源共享，提高办公效率和业务协同。 校园网：提供学校内部教学、科研和管理的网络环境。 家庭网络：连接家庭设备，实现互联网接入、家庭娱乐和智能家居控制 Q：网络核心、网络边缘的概念？ 网络核心（Network Core）是网络的中心部分，负责高速传输和交换数据包，确保数据能够在不同的网络边缘之间高效传递。核心网络通常由高性能路由器和交换机组成，提供主干传输和数据中继功能。 网络边缘（Network Edge）指的是网络中用户和设备直接接入的部分，包括终端设备、接入网络和边缘设备。它是网络中的第一跳，用户通过这里访问网络服务。 Q：网络边缘是如何接入网络核心的？&#x3D;&gt; 下面详细介绍了。 Q：现在使用的internet协议栈有几层？&#x3D;&gt; 5层 相关术语： 主机(host)（端系统，end system）：连接到网络的设备 通信链路(communication link) 分组交换机(packet switch) 分类：路由器(router)、链路层交换机(link-layer-switch) 因特网服务提供商(Internet sevice provider, ISP) TCP(transmission control protocol) IP(internet protocol) ISPs (Internet Service Providers) 因特网标准(internet standard)：由因特网工程任务组(internet engineering task force, IETF)研发。其标准文档称为请求文档(request for comment, RFC) 协议(Protocol)：定义了两个或多个通信实体之间交换的报文格式和次序，以及报文发送和&#x2F;或接受一条报文或其 他事件所采用的动作。 网络接入（Access Networks）：网络边缘接入网络核心 DSL(Digital Subscriber Line, 数字用户线) HFC( hybrid fiber coax) FTTH(Fiber To The Home，光纤到户) 拨号调制解调器：传输速率低（56 kbps），专用带宽。HFC：传输速率高（10 Mbps 到 1 Gbps 下行），共享带宽。DSL：传输速率中等（1.5 Mbps 到 100 Mbps 下行），专用带宽，受距离影响。FTTH：传输速率非常高（100 Mbps 到 10 Gbps），大多数情况下为专用带宽或部分共享，但实际体验非常高。 以太网 最流行 WIFI 广域无线网 : 5G、LTE等等。 物理媒体： 双绞铜线(Twisted-Pair Copper Wire) 同轴电缆(Coaxial Cable) 光纤(Fiber Optics) 陆地无线通信道(Terrestrial Radio Channels) 卫星无线电信道(Satellite Radio Channels) 网络协议栈： 目前因特网的协议栈为5层，但是国际标准化化组织(ISO)提出的使七层，称之为开放系统互连模型 (Open Systems Interconnection model, OSI model) 从上到下分别是： 应用层 (Application) 服务：网络应用以及它们的应用协议存留的地方 协议例子：HTTP, SMTP, FTP, DNS, P2P, Telnet 分组： 报文(message) 运输层 (Transport) （向上层）提供的服务：应用程序之间传送应用层报文 协议（实现机制）：TCP、UDP 分组：报文段 (segment) 网络层 (Network) （向上层）提供的服务：将分组从一台主机移动到另一台主机 协议（实现机制）：IP 分组：数据报 (datagram) 链路层 (Link) （向上层）提供的服务：将分组从一个节点(主机或分组交换机)移动到下一个节点 协议（实现机制）： 取决于该链路特定链路层协议 分组：帧(frame) 物理层 (physical) （向上层）提供的服务：将帧的一个一个比特从一个节点移动到下一个节点 协议（实现机制）：与实际的传输媒体有关 2. 应用层3. 传输层3.1 传输层的职责概述及其为上层提供的服务传输层负责为运行在不同主机上的应用进程提供逻辑通信，传输层的协议运行在端系统。大致的工作流程： 发送方：从应用层（主机上的应用进程）接收message，将其封装成segment ，利用网络层将数据发出。 接收方：从网络层接收数据，将segment重组成messaage，交给应用进程。 传输层是为不同主机的进程提供逻辑通信，是要依赖于下面的网络层（提供主机间的通信），由网络层找到目标主机，传输层找到目标端口，再找到对应的应用进程。传输层可以在网络层提供的服务之上进行增强，比如可靠性，顺序性，安全性等。 Internet传输层的协议包括TCP（可靠、保序）和UDP（不可靠、不保序）。 3.2 传输层的多路复用与解复用 Q：什么是传输层的多路复用与解复用？为什么需要？ 一个传输层实体会从上层接收到来自多个应用进程的message , 这些message共享这一个传输层实体，在接收方也一样。所以需要在发送方和接收方将这些message区分开，使不同的message能正确交给对应的进程。 Q：传输层是怎么实现的？ 通过在接收方将应用层传下来的message进行封装，加一些头部信息来标识，做到多路复用。在接收方根据不同的标识区分不同的报文，实现解复用。 Q：UDP和TCP在接收端解复用是不一样的，具体的区别是什么？为什么会有这样的区别？ 具体区别在于UDP解复用的时候是根据报文段的目标主机号和目标端口号定位sockte，而TCP则需要源主机，目标主机，源端口和目标端口四个字段来定位socket。产生这样区别的原因在于UDP和TCP创建socket时的所需的字段不同，而究其根本是因为TCP是面向连接的，而UDP不是。UDP不需要维护源主机到目标主机的连接信息，相当于使用UDP是只传过去就完事了，所以UDP socket不需要保留源主机号这一字段。而TCP面向连接需要握手，重传等等，是需要多次用到源主机号的信息的。 概念 发送方会从多个套接字（socket）接收来自多个进程的报文，根据套接字对应的IP地址和端口号等信息对报文段用头部加以封装(该头部信息用于以后的解复用)。 接收方根据报文段的头部信息中的IP地址和端口号将接收到的报文段发给正确的套接字(和对应的应用进程)。 UDP的多路解复用 假设主机A和主机B进行通信，A是发送方，B是接收方： 双方创建在各自的Socket。 A将message交给传输层，传输层将meaasge封装成segment，利用网络层发送给B主机。 B收到UDP segment之后，检查报文段的目标端口号，用该端口号将报文段定位给套接字。 在接收端，UDP套接字用二元组标识(目标IP地址、目标端口号)。如果两个不同源IP地址&#x2F;源端 口号的数据报，但是有相同的目标IP地址和端口号，则被定位到相同的套接字。 TCP的多路解复用 假设客户端C和服务器S： 服务器S建立起监听socket守候在80端口，等待客户端连接。C创建socket连接到服务器。 C将message交给传输层，传输层将meaasge封装成segment，利用网络层发送给服务器S。 服务器收到之后，会根据TCP segment的源主机，目标主机，源端口和目标端口来定位socket。 服务器能够在一个TCP 端口上同时支持多个 TCP套接字：每个套接字由其四元组标识（有不同的源IP和源 PORT） 在这种场景下，虽然这些socket共享一个端口，但是内核中有各自的标识。 在多线程的服务器中，会将不同的socket交给同一个进程中的不同线程进行处理。而对于单线程的服务器，书里写的是一个socket对应一个进程，也就是守候在80号端口的进程创建多个进程，将不同的socket交给他们处理。但是这样效率很低啊，有些连接不活跃，也浪费了资源，后面查阅相关资料，并不会这么做，而是会有I&#x2F;O调度模型，而是由一个进程来轮询这些连接，监听他们的状态并做出不同的处理。对于多线程的服务器也会使用线程池的机制来处理多个客户端连接。 3.3 无连接传输UDPUDP提供的是不可靠，不保序的传输，并没有在网络层提供的best effort的服务之上做太多的增强。如果要在UDP之上做可靠性传输，则需要应用层自己增加可靠性。 那为什么要存在UDP呢？ 不建立连接，传输速度快，实时性好（连接会增加延时 ） 简单：在发送端和接收端没 有连接状态 报文段的头部很小(开销小) UDP segment： UDP报文段由20字节的固定头部 + 可变长度的数据报文组成。 校验和(check sum)：检测在被传输报文段中的差错(如比特反转) 发送方： 将报文段的内容视为16 比特的整数，然后把这些16bits的整数相加（做1的补运算，进位回滚）得到校验和。再将校验和放在 UDP的校验和字段 接收方：收到数据之后进行一样的操作，进行16bits整数加法，再将计算值与检验和字段比较，不一致说明出错了，丢掉这个数据包，但是不会让发送方重传（TCP会有重传机制，UDP是没有的，丢了就丢了，它不保障）。 计算值与校验值不同：一定出错 计算值与校验值相同：也可能出错（残存错误） 了解：UDP的check sum如何生成 ？ UDP（User Datagram Protocol）的校验和字段用于确保数据在传输过程中没有发生错误。它覆盖UDP伪首部、UDP首部和数据部分。 UDP校验和计算步骤 构建伪首部： 伪首部不是真正的UDP报文的一部分，但在计算校验和时需要包含进来。伪首部包含以下字段： 源IP地址（32位） 目标IP地址（32位） 保留字段（8位，设为0） 协议号（8位，UDP为17） UDP长度字段（16位） 将伪首部、UDP首部和数据部分拼接： 把伪首部、UDP首部和数据部分拼接成一个连续的字节流。 将字节流划分为16位字（两个字节）： 如果总长度不是偶数，填充一个字节的0。 逐个相加所有16位字： 使用二进制补码求和。如果有进位，则将进位加到结果的最低有效位。 对和取反： 将最终的和按位取反，得到校验和。 填入UDP校验和字段： 将计算出的校验和填入UDP报文的校验和字段。 示例 假设我们有以下数据： 源IP地址：192.168.1.1 目标IP地址：192.168.1.2 UDP数据部分：”hello” 构建伪首部 源IP地址：192.168.1.1 -&gt; 0xC0A80101 目标IP地址：192.168.1.2 -&gt; 0xC0A80102 保留字段：0x00 协议号：17（UDP） -&gt; 0x11 UDP长度：8字节（UDP首部） + 5字节（数据） &#x3D; 13字节 -&gt; 0x000D 伪首部为： 10xC0A80101 0xC0A80102 0x0011 0x000D UDP首部 假设： 源端口：12345 -&gt; 0x3039 目标端口：80 -&gt; 0x0050 长度：13字节（UDP首部 + 数据） -&gt; 0x000D 校验和字段：暂时设为0 -&gt; 0x0000 UDP首部为： 10x3039 0x0050 0x000D 0x0000 数据部分 “hello”的ASCII码： 10x6865 0x6C6C 0x6F00 （补0以凑成偶数字节） 拼接伪首部、UDP首部和数据部分 拼接后的字节流为： 1230xC0A8 0x0101 0xC0A8 0x0102 0x0011 0x000D0x3039 0x0050 0x000D 0x00000x6865 0x6C6C 0x6F00 求和 逐个相加（16位字）： 1230xC0A8 + 0x0101 + 0xC0A8 + 0x0102 + 0x0011 + 0x000D +0x3039 + 0x0050 + 0x000D + 0x0000 +0x6865 + 0x6C6C + 0x6F00 求和结果： 1234Sum = 0xC0A8 + 0x0101 + 0xC0A8 + 0x0102 + 0x0011 + 0x000D + 0x3039 + 0x0050 + 0x000D + 0x0000 + 0x6865 + 0x6C6C + 0x6F00 = 0x25C02 (包含进位) 处理进位： 10x25C02 -&gt; 0x5C02 + 0x2 = 0x5C04 取反： 1~0x5C04 = 0xA3FB 填入校验和字段 将0xA3FB填入UDP报文的校验和字段，这样UDP报文的校验和计算完成。 3.4 面向连接的传输 TCP3.4.1. 前置知识：可靠数据传输（Reliable Data Transfer，简称RDT ）原理rdt 在应用层、传输层和数据链路层都很重要。是网络 Top 10 问题之一。 信道的不可靠特点决定了可靠数据传输协议（ rdt ）的复杂性。信道的不可靠特的不可靠性表现在： 信道存在比特差错 分组在信道中传输可能会丢失 针对比特差错，可以增加检错重传机制：（此时假设信道只存在比特差错） 可以用校验和来检测比特差错。但是当对方检测出报文出错时，会将错误报文丢弃，如何让发送方重传呢？ &#x3D;&#x3D;&gt; 通过反馈机制响应确认码：ACK 确认(ACK)：接收方显式地告诉发送方分组已被正确接收 否定确认( NAK): 接收方显式地告诉发送方分组发生了差错。发送方收到NAK后，发送方重传分组 因此发送方还需要存在一个缓存，因为出错后要重传，将报文传出去后不能立马删除，而是要缓存起来。 机制大致如下： 发送方差错控制编码、缓存 接收方使用编码检错 接收方的反馈：控制报文（ACK，NAK）：接收方 –&gt;发送方 发送方收到反馈相应的动作 但是这样的机制还存在一个致命的问题，如果ACK &#x2F; NCK反馈报文出错了怎么办？ &#x3D;&#x3D;&gt; 给发送的报文添加序号，响应报文ACK或者NCK要跟着上一次成功接收的报文序号。 比如： 发送方有pk0和pk1要发送，缓存后发送pk0 , 等待响应。 接收方成功接收pk0,检错通过，发送ACK0给对方。 ACK0如果出错了，接收方无法知道这是ACK0，变成了“阿巴阿巴”，接收方看不懂，就继续重传没有收到确认的分组pk0 接收方还是正常收到pk0,因为有加序号，它知道这是重复的分组（状态会指示希望接收到的分组的序号），于是将其丢弃，响应ACK0 这次发送方正常收到ACK0，于是开始发送ACK1… 通过引入这种机制解决响应码出错的问题。运行时如图所示： 对上诉机制的优化：使用对前一个数据单位的ACK，代替本数据单位的NAK 接收方对最后正确接收的分组发ACK，以替代NAK 接收方必须显式地包含被正确接收分组的序号 发送方收到重复的ACK（如：再次收到ack0）时， 与收到NAK采取相同的动作：重传当前分组 为后面的一次发送多个数据单位做一个准备:一次能够发送多个分组，每一个的应答都有：ACK，NACK；比较麻烦 这样可以使确认信息减少一半，协议处理简单 出错了。接收方是不需要理解ACK0或者ACK1的含义，只负责接收ACKn , 下次我就传ACK n+1 : 解决了比特差错的问题之后，再来看看如果信道还存在着丢失如何处理（此时信道具有比特差错和分组丢失） 下层信道可 能会丢失分组（数据 或ACK）：发送方发出分组后在等ACK , 接收方在等分组，但这个分组已经丢了，这样就造成了死锁。 &#x3D;&#x3D;&gt; 解决方案：超时重传，发送方等待ACK一段合理的时间，如果到时没有收到ACK就认为分组丢失了，重传。 问题：如果分组（或ACK）只是被延迟了，发送方却提前重传了？ &#x3D;&#x3D;&gt; 重传将会导致数据重复，但利用序列号已经可以处理这个问题。因此接收方必须指明被正确接收的序列号（ACKn）。 过早超时也能正常工作，但是效率较低，一半 的分组和确认是重复的； 因此设置一个合理的超时时间也是比较重要的。 以上都是假设一次只能传送一个分组，这样传输模式信道的利用率很低。如何才能同时传多个分组呢？ &#x3D;&#x3D;&gt; 流水线协议：允许发送方在未得到对方确认的情况下一次发送多个分组 必须增加序号的范围:用多个bit表示分组的序号 在发送方&#x2F;接收方要有缓冲区 发送方缓冲：未得到确认，可能需要重传； 接收方缓存：上层用户取用数据的速率 ≠ 接收到的数据速率；接收到的数据可能乱序，要进行排序交付（可靠） 两种通用的流水线协议：回退N步(GBN)和选择重传(SR) 发送方的缓冲区： 形式：内存中的一个区域，落入缓冲区的分组可以发送 功能：用于存放已发送，但是没有得到确认的分组 必要性：需要重发时可用 发送缓冲区的大小：一次最多可以发送多少个未经确认的分组（下图就是8个单位，编号 0 - 7 ， 一次最多可以发送8个未确认的分组） 停止等待协议 &#x3D;1 （stop and wait , 前面提到的） 流水线协议 &gt;1，合理的值，不能很大，链路利用率不能够超100% （pipeline） 发送缓冲区中有两类分组： 未发送的分组 已发送但是未确认的分组，这些分组只有收到ACK之后才能从缓冲区中删除（体现为滑动窗口前沿的移动） 工作流程分析：分组落入缓冲区中，初始时发送窗口（发送缓冲区内容的一个范围，是那些已发送但是未经确认分组的序号构成的空间）为0，没发送一个分组，发送窗口的后沿就向后移动1，上图发送窗口移动到了4，说明此时发生出去5个未确认的分组。当收到分组的确认后，发送窗口前沿向前移动，上图中收到了0和1号分组的确认，发送窗口前沿向前移动到了2。 接收窗口 接收窗口(receiving window)&#x3D;接收缓冲区 接收窗口用于控制哪些分组可以接收 只有收到的分组序号落入接收窗口内才允许接收 若序号在接收窗口之外，则丢弃； 接收窗口尺寸Wr&#x3D;1（GBN），则只能顺序接收。 接收窗口尺寸Wr&gt;1 (SR)，则可以乱序接收，但提交给上层的分组，要按序。 例子：Wr＝1，在0的位置；只有0号分组可以接收； 向前滑动一个，罩在1的位置，如果来了第2号分组，则丢弃； 接收窗口的滑动和发送确认： 滑动： 低序号的分组到来，接收窗口移动； 高序号分组乱序到，缓存但不交付（因为要实现rdt，不允许失序），不滑动 发送确认： 接收窗口尺寸&#x3D;1：发送连续收到的最大的分组确认（累计确认） 接收窗口尺寸&gt;1：收到分组，发送那个分组的确认（非累计确认） 想一下，这里wr &gt; 1的时候，能不能只发送窗口的最低(最左边)序号分组的那个确认，发送完再让窗口前沿往前移动？ 正常情况下发送窗口和接收窗口的交互： SR为例: 12345678910111213141516171819202122时间轴 -&gt;发送窗口 (初始): [0, 1, 2, 3]发送分组0, 1, 2, 3接收窗口 (初始): [0, 1, 2, 3]接收ACK0:发送窗口: [1, 2, 3, 4]接收窗口: [1, 2, 3, 4]接收分组2, 3:发送窗口: [1, 2, 3, 4]接收窗口: [1, 2, 3, 4]接收分组1:发送窗口: [4, 5, 6, 7]接收窗口: [4, 5, 6, 7]发送分组4, 5, 6, 7接收ACK4, 5, 6, 7:发送窗口: [8, 9, 10, 11]接收窗口: [8, 9, 10, 11] 异常情况下GBN协议的发送窗口和接收窗口的交互： 异常情况下SR协议的发送窗口和接收窗口的交互： GBN和SR的对比： 相同之处： 发送窗口&gt;1 一次能够可发送多个 未经确认的分组 不同之处： GBN:接收窗口尺寸&#x3D;1 接收端：只能顺序接收 发送端：从表现来看，一旦一个分组没有发成功，如：0,1,2,3,4 ; 假如1未成功，234都发送出去 了，要返回1再发送；GB1 发送端拥有对最老的未确认分组的定时器。只需设置一个定时器，当定时器到时时，重传所有未确认分组 SR: 接收窗口尺寸&gt;1 接收端：可以乱序接收 发送端：发送0,1,2,3,4，一旦1 未成功，2,3,4,已发送，无需重发，选择性发送1 发送方为每个未确认的分组保持一个定时器，当超时定时器到时，只是重发到时的未确认分组 3.4.2 TCP的具体实现特点概述： 点对点： 一个发送方，一个接收方 可靠的、按顺序的字节流：没有报文边界 管道化（流水线）： TCP根据调整窗口大小进行拥塞控制和流量控制 发送和接收缓存 全双工数据： 在同一连接中数据流双向流动；MSS：最大报文段大小 面向连接： 在数据交换之前，通过握手（交换控制报文）初始化发送方、接收方的状态变量 报文结构： 序号：报文段首字节的在字节流的编号 在TCP传输过程中，序号（Sequence Number）字段用于跟踪每个字节的数据顺序。当传输端（发送方）从上层接收到一个message并封装成TCP报文段时，它会使用一个随机生成的初始序号（Initial Sequence Number, ISN）。后续的TCP报文段的序号将基于这个初始序号递增。 建立连接时：在三次握手过程中，双方交换初始序号。例如，A发送的SYN报文段中包含初始序号（ISN_A），B响应的SYN-ACK报文段中包含B的初始序号（ISN_B）。 数据传输时：每个报文段的序号字段标记了该段中第一个数据字节的序号。例如，如果A的初始序号是1000，且A发送了100字节的数据，那么A发送的第一个报文段的序号字段将是1000，第二个报文段的序号字段将是1100（因为第一个报文段包含100个字节的数据）。 确认号：期望从另一方收到的下一个字节的序号；累积确认（发送方收到ACKn , 就会发出pktn） 乱序接收：没有规定接收方如何处理乱序的 报文段（但是一般都是先缓存而不会直接丢弃） TCP 协议虽然没有强制规定接收方必须采取的具体处理方式，但在实际实现中，接收方一般会采用接收缓冲区来处理乱序的报文段。这种方式确保了可靠性和有效性。如果接收方简单地丢弃乱序的报文段，确实会导致传输效率低下甚至错误，因此大多数 TCP 实现不会这样做 设置合理的超时时间： 超时时间设置为一个RTT（加权过后）+ 4被方差。可以理解为比一个往返时长再多一点点 TCP的rdt实现： TCP在IP不可靠服务的基础上 建立了rdt： 管道化的报文段（ GBN or SR） 累积确认，响应期望收到的下一个分组的序号，也可以说是顺序收到的最后一个字节的数据编号 + 1（像GBN） 单个重传定时器（像GBN） 没有规范是否可以接受乱序 通过以下事件触发重传： 超时（只重发那个最早的未确认段：SR） 重复的确认（例子：收到了ACK50,之后又收到3 个ACK50） 快速重传超时周期往往太长： 在重传丢失报文段之前的延时太长，通过重复的ACK来检测 报文段丢失。发送方通常连续发送大量 报文段。如果报文段丢失，通常会引起多个重复的ACK 发送方事件： 从应用层接收数据 –&gt; 将应用层数据分段，并为每个报文段分配序列号 –&gt; 将报文段放入发送缓冲区 –&gt; 通过网络发送 定时器与最早未确认的报文 段关联 收到对尚未确认的报文段确认，则更新已被确认的报文序号。如果当前还有未被确认的报文段，重新启动定时器 流量控制: 接收方控制发送方，不让发送方发送的太多、太快以至于让接收方的缓冲区溢出。 解决方法： 接收方在其向发送方的TCP段 头部的rwnd字段“通告”其空 闲buffer大小 大小通过socket选项 设置(典型默认大小为4096 字 节) 很多操作系统自动调整 发送方限制未确认(“in flight”)字节的个数 ≤ 接收方发送过来的值，保证接收方不会被淹没 连接管理： 在正式交换数据之前，发送方和接收方握手建立通 信关系。目的： 同意建立连接（每一方都知道对方愿意建立连接） 交换连接参数 使用两次握手可以吗？ 会存在半连接以及接收旧数据的问题， 对于场景1：如果客户端第一次的连接请求超时了，它会重发一个连接请求与服务器建立起了连接。这时前面的那个连接到了服务器这里，服务器就与它建立起了连接，给它准备连接资源。但这只是个半连接，客户端不认，只有服务器在维持，白白浪费了资源。 对于场景2：客户端与服务器建立起连接之后，连接请求和传送的数据都超时了，而且都在连接断开之后到达了服务器，服务器还是会建立连接并且接收数据。 改进：3次握手来建立起连接 这样也能解决前面的失败场景： 当旧连接到达服务器的时候，服务器会再向客户端发送一个确认连接，客户端可以拒绝或者不理，这样连接过程就不完整（只有2次握手），无法建立起连接。对于旧数据也是如此，半连接建立不起来，更别说传数据了。 在连接结束之后需要关闭连接：4次挥手 客户端，服务器分别关闭它自己这一侧的连接。客户点等待一段时间之后发现没有数据过来了，就关闭。 可以联想，两个人分别的时候，A先说要回家了： A -&gt; B : 阿B，我要回家啦 B -&gt; A ：好 B也要说一下自己走了 B -&gt; A ：那我也回家啦 A -&gt; B：ok~ 然后A目送B离开，自己也转身迈步离开。 3.5 TCP的拥塞控制前置知识：拥塞控制原理4. 网络层5. 链路层","categories":["计算机基础","计算机网络"]},{"title":"java日志库","path":"/2024/04/12/java日志库/","content":"总结一下java日志库的相关知识以及日志库的选型和基本用法 初始：一个日志库的使用例子通过一个使用log4j2 + slf4j的案例初步认识日志库的使用： 创建pom项目，引入相关依赖： 123456789101112131415161718&lt;!-- SLF4J API --&gt;&lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;slf4j-api&lt;/artifactId&gt; &lt;version&gt;1.7.32&lt;/version&gt;&lt;/dependency&gt;&lt;!-- SLF4J binding for Log4j 2 --&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.logging.log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j-slf4j-impl&lt;/artifactId&gt; &lt;version&gt;2.17.1&lt;/version&gt;&lt;/dependency&gt;&lt;!-- Log4j 2 implementation --&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.logging.log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j-core&lt;/artifactId&gt; &lt;version&gt;2.17.1&lt;/version&gt;&lt;/dependency&gt; 在项目中配置 Log4j 2 的日志记录器。创建一个 log4j2.xml 文件来配置日志记录器。 1234567891011121314&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;Configuration status=&quot;WARN&quot;&gt; &lt;Appenders&gt; &lt;Console name=&quot;Console&quot; target=&quot;SYSTEM_OUT&quot;&gt; &lt;PatternLayout pattern=&quot;%d&#123;HH:mm:ss.SSS&#125; [%t] %-5level %logger&#123;36&#125; - %msg%n&quot;/&gt; &lt;/Console&gt; &lt;/Appenders&gt; &lt;Loggers&gt; &lt;Root level=&quot;debug&quot;&gt; &lt;AppenderRef ref=&quot;Console&quot;/&gt; &lt;/Root&gt; &lt;/Loggers&gt;&lt;/Configuration&gt; 在代码中使用 SLF4J API 进行日志记录 12345678910111213package org.example;import com.sun.tools.javac.Main;import org.slf4j.Logger;import org.slf4j.LoggerFactory;public class App &#123; private static final Logger logger = LoggerFactory.getLogger(Main.class); public static void main(String[] args) &#123; logger.info(&quot;This is an info message&quot;); logger.error(&quot;This is an error message&quot;, new RuntimeException(&quot;Oops! Something went wrong.&quot;)); &#125;&#125; 查看日志输出 在这个案例中，使用了slf4j的api进行日志输出，log4j2负责在程序运行时生成并记录日志，而程序开发人员使用slf4j提供的api来指定日志的记录。 在引入依赖时，分别引入了slf4j的api ： slf4j-api、log4j2的日志功能的实现：log4j-core，以及选择log4j-slf4j-impl作为slf4j和log4j2的桥接包，让slf4j 与实现日志功能的log4j绑定。这样就可以使用slf4j的api调用log4j2提供的日志功能。这种设计模式成为门面模式（Facade Pattern 可参考：https://www.runoob.com/w3cnote/facade-pattern-3.html）。 在 Java 中，日志库通常使用门面模式来提供统一的接口，以便开发人员可以方便地切换不同的日志实现，而不必修改应用程序的代码。门面模式隐藏了底层组件的复杂性，并提供了一个简单的接口供客户端使用。 SLF4J（Simple Logging Facade for Java）是一个典型的例子，它就是使用了门面模式。SLF4J 提供了一个简单的抽象接口，开发人员可以在应用程序中使用这个接口来记录日志，而不用关心底层日志库的具体实现。然后，通过在类路径中引入相应的日志实现（如Log4j、Logback等），SLF4J 会自动找到并绑定到这些实现，从而实现日志记录功能。 日志库 — 日志门面在 Java 中，有几个常见的日志门面框架，它们为开发人员提供了统一的日志记录接口，使得在应用程序中记录日志变得更加灵活和可移植. SLF4J (Simple Logging Facade for Java)SLF4J 是一个简单的日志门面框架，它允许开发人员在应用程序中使用统一的 API 来记录日志。SLF4J 提供了一组接口，开发人员可以使用这些接口来记录日志消息，而不用关心底层日志库的具体实现。SLF4J 还提供了与不同日志实现（如Log4j、Logback等）集成的机制，通过引入相应的日志实现，可以将 SLF4J 绑定到具体的日志库上。 类似于 Common-Logging，slf4j 是对不同日志框架提供的一个 API 封装，可以在部署的时候不修改任何配置即可接入一种日志实现方案。但是，slf4j 在编译时静态绑定真正的 Log 库。使用 SLF4J 时，如果你需要使用某一种日志实现，那么你必须选择正确的 SLF4J 的 jar 包的集合（各种桥接包）。 使用SLF4j的过程中遇到的各种警告可以去https://www.slf4j.org/codes.html 里查看问题描述，基本都有解答。 使用 SLF4J 的主要步骤： 添加 SLF4J 依赖: 在项目的构建文件中（如 Maven 的 pom.xml 文件）添加 SLF4J 的依赖。 12345&lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;slf4j-api&lt;/artifactId&gt; &lt;version&gt;1.7.32&lt;/version&gt;&lt;/dependency&gt; 引入日志实现: SLF4J 是一个日志门面，它本身并不提供日志记录的实现，需要引入具体的日志实现。例如，可以引入 Logback 作为日志实现。(Logback 提供了自己的实现，并在其中包含了 SLF4J 的 API。因此不需要额外引入桥接器来使 SLF4J 与 Logback 集成，只需添加 logback-classic 依赖即可) 12345&lt;dependency&gt; &lt;groupId&gt;ch.qos.logback&lt;/groupId&gt; &lt;artifactId&gt;logback-classic&lt;/artifactId&gt; &lt;version&gt;1.2.6&lt;/version&gt;&lt;/dependency&gt; 在代码中使用 SLF4J API 进行日志记录： 12345678910111213javaCopy codeimport org.slf4j.Logger;import org.slf4j.LoggerFactory;public class MyClass &#123; private static final Logger logger = LoggerFactory.getLogger(MyClass.class); public void myMethod() &#123; logger.debug(&quot;Debug message&quot;); logger.info(&quot;Info message&quot;); logger.warn(&quot;Warning message&quot;); logger.error(&quot;Error message&quot;); &#125;&#125; Apache Commons Logging (JCL)Apache Commons Logging 是另一个常见的日志门面框架，它提供了一个简单的接口，允许开发人员在应用程序中记录日志消息。JCL 具有类似于 SLF4J 的功能，但它的设计更加简单，没有 SLF4J 那么灵活和强大。 JCL本身并不提供日志的具体实现（当然，common-logging 内部有一个 Simple logger 的简单实现，但是功能很弱，直接忽略），而是在运行时动态的绑定日志实现组件来工作（如 log4j、java.util.loggin）。 与 SLF4J 不同的是，JCL 并没有像 SLF4J 那样支持多种日志实现，而是将日志记录的实现交由第三方提供。 使用 Apache Commons Logging 的主要步骤： 添加 Apache Commons Logging 依赖: 在项目的构建文件中（如 Maven 的 pom.xml 文件）添加 Apache Commons Logging 的依赖。 12345&lt;dependency&gt; &lt;groupId&gt;commons-logging&lt;/groupId&gt; &lt;artifactId&gt;commons-logging&lt;/artifactId&gt; &lt;version&gt;1.2&lt;/version&gt;&lt;/dependency&gt; 引入日志实现: 与 SLF4J 不同，Apache Commons Logging 并不要求引入特定的日志实现。开发人员可以根据需求选择任何支持 Apache Commons Logging 的日志实现，如 Log4j、Logback 等。 在代码中使用 Apache Commons Logging API 进行日志记录： 12345678910111213import org.apache.commons.logging.Log;import org.apache.commons.logging.LogFactory;public class MyClass &#123; private static final Log logger = LogFactory.getLog(MyClass.class); public void myMethod() &#123; logger.debug(&quot;Debug message&quot;); logger.info(&quot;Info message&quot;); logger.warn(&quot;Warning message&quot;); logger.error(&quot;Error message&quot;); &#125;&#125; 需要注意的是，在使用 Apache Commons Logging 时，需要确保项目的类路径中包含了选定的日志实现（如 Log4j、Logback），否则无法正常记录日志。 日志库 — 日志实现日志实现库提供了用于记录应用程序日志的具体实现。这些库通常与日志门面框架（如SLF4J或Apache Commons Logging）配合使用，以提供统一的日志记录接口，并允许开发人员根据需要选择适当的日志实现。 Log4j 1.xLog4j 是 apache 的一个开源项目，创始人 Ceki Gulcu。它提供了成熟的日志记录功能，是 Java 领域资格最老，应用最广的日志工具。Log4j 是高度可配置的，并可通过在运行时的外部文件配置。它根据记录的优先级别，并提供机制，以指示记录信息到许多的目的地，诸如：数据库，文件，控制台，UNIX 系统日志等。 Log4j 中有三个主要组成部分： loggers - 负责捕获记录信息。 appenders - 负责发布日志信息，以不同的首选目的地。 layouts - 负责格式化不同风格的日志信息。 优点： 成熟稳定: Log4j 1.x 是一个成熟的日志框架，在很长一段时间内被广泛应用和验证。 功能丰富: 提供了丰富的功能和配置选项，适用于各种日志需求。 缺点： 性能不如后续版本: 相比于 Log4j 2 和 Logback，性能略逊一筹。 维护状态: Log4j 1.x 的维护状态已经停止，不再推荐新项目使用 使用方法： 添加 Log4j 1.x 的依赖到项目中。 创建 log4j.properties 或 log4j.xml 配置文件，配置日志记录器、输出目标等。 在代码中使用 Log4j 1.x 的 API 进行日志记录。 Log4j 2维护 Log4j 的人为了性能又搞出了 Log4j2。Log4j2 和 Log4j1.x 并不兼容，设计上很大程度上模仿了 SLF4J&#x2F;Logback，提供了异步日志记录、插件架构、灵活的配置选项等特性，被广泛用于 Java 应用程序的日志记录，性能上也获得了很大的提升。 Log4j2 也做了 Facade&#x2F;Implementation 分离的设计，分成了 log4j-api 和 log4j-core。 优点： 高性能: Log4j 2 针对性能进行了优化，特别是在异步日志记录方面，可以提供较高的吞吐量。 灵活的配置: Log4j 2 提供了丰富的配置选项，允许开发人员通过 XML、JSON、YAML 或编程方式配置日志记录器。 插件架构: Log4j 2 使用插件架构，可以轻松地扩展功能，添加新的日志输出目标、格式化器等。 支持多种日志级别: 支持调试、信息、警告、错误、致命等多种日志级别。 与 SLF4J 集成: 可以与 SLF4J 集成，使得在应用程序中记录日志更加灵活和可移植。 缺点： 配置相对复杂: 配置文件的格式较为复杂，需要一定的学习成本。 资源占用较高: 相对于其他日志库，可能会占用较多的资源。 使用方法： 添加 Log4j 2 的依赖到项目中。 创建 log4j2.xml 或 log4j2.yaml 等配置文件，配置日志记录器、输出目标等。 在代码中使用 SLF4J 或 Log4j 2 的 API 进行日志记录。 LogbackLogback 是由 Log4j 的作者开发的日志框架，被设计成 Log4j 的改进版。它具有与 Log4j 相似的功能，但在性能上有所提升。Logback 提供了异步日志记录、灵活的配置选项等特性，同时与 SLF4J 集成，使得在应用程序中记录日志变得更加灵活和可移植。 logback 当前分成三个模块：logback-core、logback-classic 和 logback-access。 logback-core ：是其它两个模块的基础模块。 logback-classic ：是 log4j 的一个 改良版本。此外 logback-classic 完整实现 SLF4J API 使你可以很方便地更换成其它日志系统如 log4j 或 JDK14 Logging。 logback-access ： 访问模块与 Servlet 容器集成提供通过 Http 来访问日志的功能。 优点： 性能优秀: Logback 是 Log4j 的改进版，性能较好，特别是在并发环境中表现突出。 简单配置: Logback 的配置相对简单，易于上手。 与 SLF4J 集成: 可以与 SLF4J 集成，提供了统一的日志记录接口。 缺点： 社区活跃度较低: 相比于 Log4j 2，Logback 的社区活跃度可能稍低。 插件数量相对较少: 插件数量不及 Log4j 2 多。 使用方法： 添加 Logback 的依赖到项目中。 创建 logback.xml 配置文件，配置日志输出目标、格式化器等。 在代码中使用 SLF4J 或 Logback 的 API 进行日志记录。 java.util.logging (JUL)JUL是 Java 平台自带的日志框架，也称为 JDK 日志框架。它提供了基本的日志功能，包括日志记录器、日志处理器、日志格式化器等，可以在大多数 Java 应用程序中直接使用，而无需额外的依赖。 虽然是官方自带的log lib，JUL的使用确不广泛。原因有以下几点： JUL从JDK1.4 才开始加入(2002年)，当时各种第三方log lib已经被广泛使用了 JUL早期存在性能问题，到JDK1.5上才有了不错的进步，但现在和Logback&#x2F;Log4j2相比还是有所不如 JUL的功能不如Logback&#x2F;Log4j2等完善，比如Output Handler就没有Logback&#x2F;Log4j2的丰富，有时候需要自己来继承定制，又比如默认没有从ClassPath里加载配置文件的功能 使用方法： 在代码中直接使用 java.util.logging 包中的类进行日志记录。 可以通过配置文件或代码配置日志记录器、处理器等。 日志库的选型与具体使用 使用日志库通常涉及3个基本步骤： 引入对应的依赖 配置日志实现组件 使用API 重点在于引入依赖，因为配置和api的使用不同的日志门面和日志实现基本相同。 正确引入对应的依赖 使用logback + slf4j 直接引入logback-classic 即可 123456&lt;dependency&gt; &lt;groupId&gt;ch.qos.logback&lt;/groupId&gt; &lt;artifactId&gt;logback-classic&lt;/artifactId&gt; &lt;version&gt;1.4.11&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt;&lt;/dependency&gt; logback-classic会自动将 slf4j-api 和 logback-core也添加到项目中: slf4j + log4j 直接引入slf4j-log4j12 它会一起引入slf4j-api 和 log4j 12345&lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;slf4j-log4j12&lt;/artifactId&gt; &lt;version&gt;1.7.21&lt;/version&gt;&lt;/dependency&gt; 也可以分别引入slf4j-api、log4j-slf4j-impl、log4j-core，比较麻烦，还要注意版本有没有冲突 slf4j + jul 直接引入slf4j-jdk14，它会自动添加相关依赖 12345&lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;slf4j-jdk14&lt;/artifactId&gt; &lt;version&gt;2.0.12&lt;/version&gt;&lt;/dependency&gt; slf4j兼容非slf4j组件 — 桥接假如应用程序所调用的组件当中已经使用了 common-logging，这时需要 jcl-over-slf4j.jar 把日志信息输出重定向到 slf4j-api，slf4j-api 再去调用 slf4j 实际依赖的日志组件。这个过程称为桥接。下图是官方的 slf4j 桥接策略图 无论项目中使用的是 common-logging 或是直接使用 log4j、java.util.logging，都可以使用对应的桥接 jar 包来解决兼容问题。 slf4j 兼容 common-logging 12345&lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;jcl-over-slf4j&lt;/artifactId&gt; &lt;version&gt;1.7.12&lt;/version&gt;&lt;/dependency&gt; slf4j 兼容 log4j 12345&lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;log4j-over-slf4j&lt;/artifactId&gt; &lt;version&gt;1.7.12&lt;/version&gt;&lt;/dependency&gt; slf4j 兼容 java.util.logging 12345&lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;jul-to-slf4j&lt;/artifactId&gt; &lt;version&gt;1.7.12&lt;/version&gt;&lt;/dependency&gt; spring 集成 slf4j 做 java web 开发，基本离不开 spring 框架。很遗憾，spring 使用的日志解决方案是 common-logging + log4j。 所以，你需要一个桥接 jar 包：logback-ext-spring。 123456789101112131415&lt;dependency&gt; &lt;groupId&gt;ch.qos.logback&lt;/groupId&gt; &lt;artifactId&gt;logback-classic&lt;/artifactId&gt; &lt;version&gt;1.1.3&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.logback-extensions&lt;/groupId&gt; &lt;artifactId&gt;logback-ext-spring&lt;/artifactId&gt; &lt;version&gt;0.1.2&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;jcl-over-slf4j&lt;/artifactId&gt; &lt;version&gt;1.7.12&lt;/version&gt;&lt;/dependency&gt; 使用日志库时的注意事项 不要直接使用日志实现组件 使用日志门面（如 SLF4J）能使代码与特定的日志实现解耦。如果将来需要更换或升级日志实现，只需更改配置和依赖，而不需要修改代码。 直接使用日志实现组件会导致代码与特定的日志实现紧密耦合在一起。这样的设计不利于代码的可移植性和重用性。 在一个大型的应用或者多模块项目中，可能会使用多个库或框架，它们可能依赖于不同版本的同一个日志库。如果直接使用日志实现，可能会导致类路径冲突和版本冲突，增加了解决问题的难度。 使用日志门面可以更容易地进行性能优化和适配。日志门面可以为不同的日志实现提供适配器，以优化性能或适应特定的环境。 通过日志门面，可以更容易地统一日志的格式和风格，无论使用哪种日志实现。这有助于提高日志的可读性和统一性。 只引入一个具体的日志实现组件 解决dubbo那个issue的时候就遇到了这个问题，有一个模块在pom里直接引入了logf42作为日志实现，而引入的dubbo-zookeeper-curator5-spring-boot-starter里面又没有排除掉logback。导致在类路径里引入了多个日志实现。就会有警告，并且slf4j会自己选择其中一个作为实现。 那这么看来，应该算是dubbo-zookeeper-curator5-spring-boot-starter这个stater的问题，它应该把logback的依赖设置为optional，不应该直接传递过来。（可以去提个issue并解决） 如果引入多个日志实现组件，可能会导致类路径冲突、不一致的日志行为或者混乱的日志输出。这可能会增加维护成本，并且在解决问题时可能会变得非常困难。 每增加一个日志实现组件都会增加项目的复杂性。不同的日志实现可能有自己特有的配置、API和行为，这会增加学习成本和维护难度。 只使用一个日志实现组件可以确保日志输出的一致性和统一性，有助于更好地理解和分析日志信息。 具体的日志实现依赖应该设置为optional和使用runtime scope 在项目中，Log Implementation的依赖强烈建议设置为runtime scope，并且设置为optional。例如项目中使用了 SLF4J 作为 Log Facade，然后想使用 Log4j2 作为 Implementation，那么使用 maven 添加依赖的时候这样设置: 1234567891011121314&lt;dependency&gt; &lt;groupId&gt;org.apache.logging.log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j-core&lt;/artifactId&gt; &lt;version&gt;$&#123;log4j.version&#125;&lt;/version&gt; &lt;scope&gt;runtime&lt;/scope&gt; &lt;optional&gt;true&lt;/optional&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.logging.log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j-slf4j-impl&lt;/artifactId&gt; &lt;version&gt;$&#123;log4j.version&#125;&lt;/version&gt; &lt;scope&gt;runtime&lt;/scope&gt; &lt;optional&gt;true&lt;/optional&gt;&lt;/dependency&gt; 设为optional，依赖不会传递，这样如果你是个lib项目，然后别的项目使用了你这个lib，不会被引入不想要的Log Implementation 依赖；Scope设置为runtime，是为了防止开发人员在项目中直接使用Log Implementation中的类，而不使用Log Facade中的类。 必要时排除依赖的第三方库中的Log Impementation依赖 这是很常见的一个问题，第三方库的开发者未必会把具体的日志实现或者桥接器的依赖设置为optional，然后你的项目继承了这些依赖——具体的日志实现未必是你想使用的，比如他依赖了Log4j，你想使用Logback，这时就很尴尬。另外，如果不同的第三方依赖使用了不同的桥接器和Log实现，也极容易形成环。 这种情况下，推荐的处理方法，是使用exclude来排除所有的这些Log实现和桥接器的依赖，只保留第三方库里面对Log Facade的依赖。 这个也在上次解决issue的时候见到了： 12345678910&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter&lt;/artifactId&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;artifactId&gt;spring-boot-starter-logging&lt;/artifactId&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt;&lt;/dependency&gt; 这个为了排除掉springboot自带的日志实现logback。因为项目中使用的是log4j2。如果不exclude的话，可能会造成日志冲突。 当时是log4j-to-slf4j在springboot-starter里面有引入；log4j-slf4j2-impl在spring-boot-starter-log4j2里面。这两个桥接器造成了冲突。","tags":["java"],"categories":["编程学习","java"]},{"title":"test","path":"/2024/04/11/test/","content":"咋样才能更方便地使用图片啊： 使用全局文件夹&#x2F;assets&#x2F;，使用markdown的语法![](url) 配置post_asset_folder: true 每次new 文章的时候会创建一个同名资源文件夹。 直接使用相对路径 使用插件的语法 123&#123;% asset_path slug %&#125;&#123;% asset_img slug [title] %&#125;&#123;% asset_link slug [title] %&#125; {} 图片是放在assets还是当前的资源文件夹 使用emoji："},{"title":"记录第一次在开源社区提交pr的过程","path":"/2024/04/11/记录第一次在开源社区提交pr/","content":"记录解决dubbo社区issue的过程！这是一个good-first-issue，比较适合社区的新人。我刚看到issue描述的时候觉得很容易完成，更新相关依赖不就可以了，几分钟就搞定了吧。然而当我第一次push到仓库进行ci测试之后，才真正见识到了依赖版本冲突的可怕… issue描述https://github.com/apache/dubbo/issues/13862 component： Java Samples (apache&#x2F;dubbo-samples) description： Dubbo Samples switched the required JDK version to OpenJDK17. Dubbo Samples are entirely switched to SpringBoot3. related： https://github.com/apache/dubbo/discussions/13841 关于issue的讨论，用处不大 https://github.com/apache/dubbo-samples/pull/1057 别人提交过的pr，可以参考 https://github.com/apache/dubbo-samples/pull/1089 背景调研 apache&#x2F;dubbo-samples仓库是干什么的？ 用来演示dubbo的用途。里面有很多个子项目，分别展示了dubbo从基础到进阶的各种用法。 利用这些sample通过docker来进行dubbo的集成测试。 这些samples怎么运行？ 直接clone之后找到自己想要的某个sample，单独构建运行一个子项目。不要直接运行整个项目，太大了。 12$ cd 1-basic/dubbo-samples-spring-boot$ mvn clean package 具体描述issue是要做什么事情? 把这些sample的jdk版本更新到jdk17。之前可能用的比较老的版本。springboot3最低支持JDK 17 ，不再支持JDK 17之前的版本。 所以这些samples是为了更新springboot的版本，比如springboot3。因此也要更新jdk版本。 猜测sample里的项目大多都是基于springboot开发的，因此更新这个仓库里的所有项目需要很大的工作量。他才会让我们只负责单独的某个或几个模块。 相关的那个pr是做了什么事情？ 把相关项目的pom文件的java版本target和source改为17，springboot版本改为3.2： 解决过程 找到项目对应模块中使用到springboot的地方，将pom文件的属性改一下就行。 测试失败！！！ springcloud的服务提供者provider可以正常启动。 Dubbo Client服务调用方无法正常启动： 查看日志发现报错：在注册中心里面找不到provider。 将分支切换回master,使用原来的测试用例启动provider时，可以在nacos的控制台查看注册的服务列表： 这个服务是有注册上的。而报错的分支却没有注册服务。因此，问题出在更新代码之后provider没有办法正常在nacos注册。 springboot和springcloud是有版本的对应关系的，我刚刚只更新了springboot的版本而没有同步更新cloud的版本，导致出现了问题。 在spring的官网查看spring官网查看springboot和springcloud的版本对应关系： 查看springboot与springcloudalibaba的版本对应关系： https://github.com/alibaba/spring-cloud-alibaba/wiki/%E7%89%88%E6%9C%AC%E8%AF%B4%E6%98%8E#%E7%BB%84%E4%BB%B6%E7%89%88%E6%9C%AC%E5%85%B3%E7%B3%BB 将spring-cloud和spring-cloud-alibaba的版本更新后，全部正常。提交看看CI测试。不通过。 wc,把另一个模块给忘了。赶紧把sc-call-dubbo的模块也改了 : 以来都改了之后，手动调用是正常的。 但是测试用例却通过不了： 1234567891011121314151617181920212223242526@Test public void test() &#123; String url = String.format(&quot;http://%s:8099/dubbo/rest/test1&quot;, consumerAddress); RestTemplate restTemplate = new RestTemplate(); UserList userList = restTemplate.getForObject(url, UserList.class); assert userList != null; Assert.assertEquals(1, userList.getUsers().size()); &#125; public class UserList &#123; private List&lt;User&gt; users; public UserList() &#123; users = new ArrayList&lt;&gt;(); &#125; public List&lt;User&gt; getUsers() &#123; return users; &#125; public void setUsers(List&lt;User&gt; users) &#123; this.users = users; &#125; &#125; 报错信息提示：在将调用返回的结果json转为UserList的时候发生了错误。 自己在浏览器调用应用的服务拿到返回数据： 1[&#123;&quot;id&quot;:1,&quot;name&quot;:&quot;Dubbo provider!&quot;&#125;] 这是一个json数组，里面包含一个对象。这个对象有两个属性 id 和 name。对应着sample里的实体类User。用UserList来接收json反序列化后的对象，即一个对象里面有一个类型为数组的属性。与返回数据不匹配啊，直接返回的是一个数组。把测试用例改一下试一试： 1234567891011@Test public void test() &#123; String url = String.format(&quot;http://%s:8099/dubbo/rest/test1&quot;, consumerAddress); RestTemplate restTemplate = new RestTemplate(); // 直接解析为 List&lt;User&gt; List&lt;User&gt; userList = restTemplate.getForObject(url, List.class); Assert.assertNotNull(userList); Assert.assertEquals(1, userList.size()); &#125; ok,本地测试通过了。但是ci测试还是不通过。 55个测试只通过了49个。 查看详细的日志，一直是这两个模块environment-keys和call-back的问题: Dubbo 3.3 &#x2F; Test Result (Java17) Dubbo 3.3 &#x2F; Test Result (Java21) integration Test (Java17, Job3) integration Test (Java21, Job3) 日志框架出现了多个绑定：https://www.slf4j.org/codes.html#multiple_bindings integration Test (Java17, Job4) integration Test (Java21, Job4) Integration Test Job3失败的原因在于：environment-keys-provider出现了多个日志实现。 Integration Test Job4失败的原因在于：callback-consumer无法连接找到provider提供的服务。 而test result失败就是因为有了这两个job的失败，在merge test result时会出现两个失败的case。 在java17和java21版本各有3个failures，所以一共有6个不通过的ci测试。 只要解决Integration Test Job3和Integration Test Job4即可。 着手解决job3: 123456789101112131415161718192021SLF4J(W): Class path contains multiple SLF4J providers.SLF4J(W): Found provider [org.apache.logging.slf4j.SLF4JServiceProvider@3cd1f1c8]SLF4J(W): Found provider [ch.qos.logback.classic.spi.LogbackServiceProvider@3a4afd8d]SLF4J(W): See https://www.slf4j.org/codes.html#multiple_bindings for an explanation.SLF4J(I): Actual provider is of type [org.apache.logging.slf4j.SLF4JServiceProvider@3cd1f1c8]Exception in thread &quot;main&quot; java.lang.ExceptionInInitializerError\tat org.apache.dubbo.samples.environment.keys.provider.ProviderApplication.main(ProviderApplication.java:28)Caused by: org.apache.logging.log4j.LoggingException: log4j-slf4j2-impl cannot be present with log4j-to-slf4j\tat org.apache.logging.slf4j.Log4jLoggerFactory.validateContext(Log4jLoggerFactory.java:70)\tat org.apache.logging.slf4j.Log4jLoggerFactory.newLogger(Log4jLoggerFactory.java:50)\tat org.apache.logging.slf4j.Log4jLoggerFactory.newLogger(Log4jLoggerFactory.java:33)\tat org.apache.logging.log4j.spi.AbstractLoggerAdapter.getLogger(AbstractLoggerAdapter.java:53)\tat org.apache.logging.slf4j.Log4jLoggerFactory.getLogger(Log4jLoggerFactory.java:33)\tat org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:422)\tat org.apache.commons.logging.LogAdapter$Slf4jAdapter.createLocationAwareLog(LogAdapter.java:121)\tat org.apache.commons.logging.LogAdapter.createLog(LogAdapter.java:95)\tat org.apache.commons.logging.LogFactory.getLog(LogFactory.java:67)\tat org.apache.commons.logging.LogFactory.getLog(LogFactory.java:59)\tat org.springframework.boot.SpringApplication.&lt;clinit&gt;(SpringApplication.java:202)\t... 1 more 在应用程序中，发现了多个 SLF4J 的提供者， org.apache.logging.slf4j.SLF4JServiceProvider@3cd1f1c8 ch.qos.logback.classic.spi.LogbackServiceProvider@3a4afd8d 这只是警告，它也自己选了ch.qos.logback.classic.spi.LogbackServiceProvider@3a4afd8d这个实现。 在错误消息指出了 log4j-slf4j2-impl 和 log4j-to-slf4j 不能同时存在。这是因为这两个依赖提供了不同的方式来桥接 Log4j 和 SLF4J，而它们之间可能会发生冲突。解决这个问题的方法通常是排除其中一个冲突的依赖，以确保只有一个 SLF4J 的实现被加载。 idea里打开dubbo.samples.environment.keys.provider模块，在maven插件里搜索这两个依赖： 发现log4j-to-slf4j是在spring-boot-starter里的spring-boot-logging引入的 而log4j-slf4j2-impl则是在spring-boot-starter-log4j2里引入的。 删除它们中的任意一个依赖就可以了。 ps : 我当时思路还没这么清晰，在引入spring-boot-starter的时候把spring-boot-logging排除掉： (其实在这里还有一个问题，在dubbo.samples.environment.keys.provider的父模块里进行依赖管理(标签)的时候，已经把这个spring-boot-starter-logging给排除了，子模块直接引入spring-boot-starter的时候为什么还是把它引进来了？) 123456789101112&lt;!-- spring starter --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;$&#123;spring-boot.version&#125;&lt;/version&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;artifactId&gt;spring-boot-starter-logging&lt;/artifactId&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;/dependency&gt; 还顺手把spring-boot-starter-log4j2依赖给删了，本来以为这样会破坏它的日志系统，因为删掉了日志实现组件。但是这是没问题的，因为在dubbo-zookeeper-curator5-spring-boot-starter这个依赖里面它引入了logback-classic ，所以在这个模块里的日志库使用的是log4j + logback。 奈斯奈斯！！！job3解决！！！ 接下来解决job4: 1234567891011121314---------------------------------------------------------- dubbo-samples-callback log: callback-consumer.log----------------------------------------------------------Start at: 2024-04-09 12:34:51Waiting ports before run test ..checking tcp ports: callback-provider:2181;callback-provider:20880;, start at: 0, timeout: 180checking tcp port [callback-provider:2181] ...telnet: Unable to connect to remote host: Connection refusedTrying 192.168.48.2...telnet: Unable to connect to remote host: Connection refusedTrying 192.168.48.2...Trying 192.168.48.2...Connected to callback-provider.Escape character is &#x27;^]&#x27;. callback-consumer找不到provider,连接超时了哇。猜测是provider无法正常在Zookeeper注册。 运行一下测试用例,发现了一个错误信息： 12312:59:33.141 [ZooKeeper Server Starter] INFO org.apache.zookeeper.audit.ZKAuditProvider -- ZooKeeper audit is disabled.12:59:33.688 |-ERROR [main] org.apache.dubbo.common.Version:111 -| [DUBBO] Inconsistent version 3.2.3 found in dubbo-zookeeper-curator5-spring-boot-starter from file:/D:/devTool/Maven/maven-repository/org/apache/dubbo/dubbo-zookeeper-curator5-spring-boot-starter/3.2.3/dubbo-zookeeper-curator5-spring-boot-starter-3.2.3.jar!/META-INF/versions/dubbo-zookeeper-curator5-spring-boot-starter, expected dubbo-common version is 3.3.0-beta.1, dubbo version: 3.3.0-beta.1, current host: 169.254.123.149, error code: 0-12. This may be caused by , go to https://dubbo.apache.org/faq/0/12 to find instructions. 12:59:33.692 |-ERROR [main] org.apache.dubbo.common.Version:111 -| [DUBBO] Inconsistent git build commit id 8256e5646f5589bced5c458db586cae70cd16e2f found in dubbo-zookeeper-curator5-spring-boot-starter from file:/D:/devTool/Maven/maven-repository/org/apache/dubbo/dubbo-zookeeper-curator5-spring-boot-starter/3.2.3/dubbo-zookeeper-curator5-spring-boot-starter-3.2.3.jar!/META-INF/versions/dubbo-zookeeper-curator5-spring-boot-starter, expected dubbo-common version is 0c9d5e2f7383760018db38877863d6fc16fd6689, dubbo version: 3.3.0-beta.1, current host: 169.254.123.149, error code: 0-12. This may be caused by , go to https://dubbo.apache.org/faq/0/12 to find instructions. 这个错误消息表明在项目中发现了不一致的 Dubbo 版本信息。Dubbo 在运行时发现了一个与预期版本不一致的组件，这可能导致一些问题。在日志中，Dubbo 发现了一个名称为 dubbo-zookeeper-curator5-spring-boot-starter 的组件，其版本为 3.2.3，但是 Dubbo 期望的版本是 3.3.0-beta.1。 soga，原来是starter的版本跟dubbo版本冲突了！！ 那就把dubbo-zookeeper-curator5-spring-boot-starter改为3.3.0-beta.1 完美解决！！！ 总结梳理一遍流程： springcloud模块： 更新相关依赖 更新test case 更新test-case configuration environment-keys的consumer和provider模块： 删除spring-boot-starter-log4j2依赖 （其实排除dubbo-zookeeper-curator5-spring-boot-starter里的logback更好） 在spring-boot-starter里面排除logging callback模块: 修改dubbo-zookeeper-curator5-spring-boot-starter版本 修改日志依赖 可是我刚刚去测试了一遍如果不删除spring-boot-starter-log4j2的依赖，callback和environment-keys模块还是会有那个警告… 那个警告来自slf4j,提示你在项目的classpath里面引入了两个日志实现log4j2和logback,移除掉其中一个就好了。 但是在maven的依赖树里查找就不存在 log4j-slf4j2-impl 和 log4j-to-slf4j 同时存在的问题。 总结一下这次解决issue的收获： 做事情要专注并且持续地做完。不能像上次解决issue的时候三天打鱼两天晒网，断断续续地，最后没有完成任务。 看日志要有耐心并且仔细，快速定位到问题。不要日志随便一扫就盲目靠感觉去试，那样子成功了只是运气好，一定要学会好好分析问题。 需要学习一下GitHub ci的相关知识，这样有助于更好理解ci测试failure是为什么 需要重新复习java的日志库，日志系统和日志门面又忘了。刚好这次遇到了日志框架冲突的问题，可以结合这个例子加深一下印象。 Appendix过了5天终于把我的pr合并啦~ 还给我点了个赞，非常开心！！！ 又回来这个仓库看看，发现项目中使用的日志框架都是slf4j + log4j2。而在我提交的pr中，把两个模块的日志框架改成了logback + slf4j。这和项目不统一，应该修改。同时在项目中也存在着许多日志警告的问题，大都是在引入dubbo-zookeeper-curator5-spring-boot-starter的时候没有排除掉对应的依赖。 下面列出存在问题的模块： dubbo-samples-cache 、dubbo-samples-group 、dubbo-samples-mock 、dubbo-samples-notify 、dubbo-samples-rpccontext 、dubbo-samples-service-discovery 、dubbo-samples-spring-security 、dubbo-samples-validation ：存在多个日志实现的警告（原因：引入dubbo-zookeeper-curator5-spring-boot-starter没有排除logback；这个版本3.3.0-beta.2-SNAPSHOT已经将日志依赖设置为optional，3.3.0-beta.1 则会存在这个问题） dubbo-samples-callback 、 dubbo-samples-environment-keys 使用了logback , 而不是项目统一的log4j2 对于存在多个日志实现的警告，解决方法有两种： 在引入依赖时手动排除掉logback的相关依赖 引入新版本的dubbo-zookeeper-curator5-spring-boot-starter , 及3.3.0-beta.2，这个版本已经不会自动将日志实现一起引入。 对于日志框架不统一的问题，需要自己再手动解决： 在依赖管理中排除dubbo-zookeeper-curator5-spring-boot-starter中的logback相关依赖 引入slf4j2的相关依赖","tags":["dubbo"],"categories":["编程学习","参与开源社区"]},{"title":"git的原理简介及其常用命令总结","path":"/2024/04/03/git的原理简介及其常用命令总结/","content":"总结一下git的基本使用以及记录一些自己遇到的场景 基本原理简介版本控制 本地版本控制：采用某种简单的数据库来记录文件的历次更新差异。 缺点：无法让在不同系统上的开发者协同工作。 集中式版本控制：有一个单一的集中管理的服务器，保存所有文件的修订版本，而协同工作的人们都通过客户端连到这台服务器，取出最新的文件或者提交更新。 缺点：中央服务器可能会造成单点故障，整个项目的历史记录被保存在单一位置，就有丢失所有历史更新记录的风险。 分布式版本控制：客户端并不只提取最新版本的文件快照，而是把代码仓库完整地镜像下来。 这么一来，任何一处协同工作用的服务器发生故障，事后都可以用任何一个镜像出来的本地仓库恢复。 因为每一次的克隆操作，实际上都是一次对代码仓库的完整备份。 基本特点 直接记录快照，而非差异比较 例如 CVS、Subversion、Perforce 等等，这些系统是以“文件差异（增量）”的方式来保存文件历史。简单来说，它们存储的是文件在不同版本之间的变化（变更列表）。每个文件被视作一个基础版本，随后记录的是它每次被修改时的“增量”，也就是每次变化的内容。这种方式让文件的每个版本都可以回溯到之前的变动。 Git 不使用增量存储，而是每次提交时对所有文件创建一个快照（类似“拍照”）。Git 会将这些快照链接起来，组成一个完整的历史记录。如果文件没有发生变化，Git 并不会重复存储这个文件，而是建立一个指向之前快照的链接。因此，Git 的存储方式更高效。这种快照流的存储方式带来了 Git 的强大功能，比如快速切换分支和轻松回溯历史记录，因为每个提交点实际上是项目在该时间点的完整状态。 性能与结构的不同： Git 的快照方式本质上更类似于一个轻量级的文件系统，它通过快照的索引和链接构建出完整的项目历史。每个提交包含一个指向全部文件状态的完整快照，这让 Git 能够在复杂操作中高效定位到整个项目的完整状态，而传统增量系统则更依赖于逐步比对增量差异。 分支和切换效率： Git 的快照方式让分支成为项目的一个独立路径，而不是基于变更列表进行拼凑，因此可以实现快速分支和切换。在传统的版本控制中，分支是基于“变更”的集合，分支切换时需要逐个文件地应用或撤销变更，因而更慢。 版本回溯： Git 中的每次提交即为项目的完整状态，这意味着 Git 能直接定位任意版本，避免了增量变更系统中的多次回溯，适合更大规模的数据管理 近乎所有操作都是本地执行 Git 的项目历史在本地存储，这意味着在查看提交历史或对比当前版本与旧版本的改动时，Git 直接读取本地数据库。无需连接服务器或下载额外数据，操作几乎是瞬时完成的。 Git 的本地历史还意味着你可以在无网络连接的情况下执行几乎所有的 Git 操作。例如，修改代码、查看历史、创建分支、进行提交等。只需在有网络时同步即可。这在离线工作或网络不稳定（如旅行途中或 VPN 连接问题时）极为方便。 Git 保证完整性 Git 中所有数据在存储前都计算校验和，然后以校验和来引用。 这意味着不可能在 Git 不知情时更改任何文件内容或目录内容。 这个功能建构在 Git 底层，是构成 Git 哲学不可或缺的部分。 若在传送过程中丢失信息或损坏文件，Git 就能发现。 Git 用以计算校验和的机制叫做 SHA-1 散列。 这是一个由 40 个十六进制字符（0-9 和 a-f）组成字符串，基于 Git 中文件的内容或目录结构计算出来。 SHA-1 哈希看起来是这样： 124b9da6552252987aa493b52f8696cd6d3b00373 Git 中使用这种哈希值的情况很多，你将经常看到这种哈希值。 实际上，Git 数据库中保存的信息都是以文件内容的哈希值来索引，而不是文件名。 Git 三大区域Git 有三种状态，文件可能处于其中之一：已提交（committed）、已修改（modified）和已暂存（staged）。 已提交表示数据已经安全的保存在本地数据库中。 已修改表示修改了文件，但还没保存到数据库中。 已暂存表示对一个已修改文件的当前版本做了标记，使之包含在下次提交的快照中。 由此引入 Git 项目的三个工作区域的概念：工作目录、暂存区域以及Git 仓库。 Git 仓库目录是 Git 用来保存项目的元数据和对象数据库的地方。 这是 Git 中最重要的部分，从其它计算机克隆仓库时，拷贝的就是这里的数据。 工作目录是对项目的某个版本独立提取出来的内容。 这些从 Git 仓库的压缩数据库中提取出来的文件，放在磁盘上以供使用或修改。 暂存区域是一个文件，保存了下次将提交的文件列表信息，一般在 Git 仓库目录中。 内部结构在 Git 中，每次提交都会生成唯一的快照，而这些快照由四种核心对象构成：Blob、Tree、Commit 和 Tag。Git 使用这些对象及其关联关系来保存项目状态，并通过 SHA-1 哈希确保对象的唯一性。 1. Blob Blob 是文件数据的基本存储单元，保存文件的内容而不是元数据（如文件名和权限）。 当文件被添加到 Git 时，Git 会将文件内容转换为 Blob 对象，并生成一个基于内容的 SHA-1 哈希作为 Blob 的唯一标识符。 如果文件内容没有变化，即使文件在不同提交中重复出现，Git 也只会保存一份 Blob，从而节省存储空间。 2. Tree Tree 用于存储目录结构和文件关系，相当于文件夹。它包含多个文件（Blob）和子目录（Tree）的引用。 每个 Tree 对象保存了当前目录下的 Blob 和 Tree，并存储文件的元数据（如文件名和权限）。 Git 通过树对象可以迅速重建出整个项目的目录结构，从而为提交生成文件系统快照。 3. Commit Commit 是 Git 中保存项目版本快照的对象，包含以下信息： 指向根目录的 Tree 对象； 提交者信息（姓名、邮箱和时间）； 提交的 SHA-1 哈希； 一个或多个父提交（单个提交通常有一个父提交，合并提交会有多个父提交）。 每次提交生成唯一的 SHA-1 哈希值，这个哈希不仅仅用于标识当前提交的内容，还能通过父提交关联生成提交历史。 4. Tag Tag 为提交创建一个别名，通常用于标识重要的发布版本（如 v1.0、v2.0 等）。 标签对象包含提交对象的哈希、标签名称及描述信息。 标签有两种类型： 轻量标签：直接指向某个提交，不包含额外的元数据； 附注标签：可以包含签名、附加信息等，是一种更正式的标记。 SHA-1 哈希与版本管理Git 使用 SHA-1 哈希来确保版本控制中的对象具有唯一性并实现数据完整性。SHA-1 哈希是一种加密散列函数，它会将输入内容（文件、目录或提交）生成一个长度为 40 个字符的十六进制字符串（160 位）。在 Git 中，每一个对象类型（Blob、Tree、Commit、Tag）都生成唯一的 SHA-1 哈希值，从而实现对内容的完整标识。这一设计是 Git 保证内容一致性和防篡改的基础。 唯一标识：哈希值是根据对象内容计算的，所以即使文件或目录被放在不同路径或仓库中，只要内容相同，生成的哈希值也完全相同。不同的内容，即使只是一个字符的改动，也会生成完全不同的哈希。 防篡改性：由于 SHA-1 哈希算法的特性，任何对文件内容的修改都会导致哈希值的变化。因此，一旦内容被更改，原先的哈希值就不再匹配，这让 Git 能够立即检测出更改。 效率：SHA-1 哈希允许 Git 通过哈希值快速地定位对象，不需要遍历整个仓库，从而大大提高版本检索的效率。 分支和合并模型对工作目录的文件做些修改后再次提交，那么这次产生的提交对象会包含一个指向上次提交对象（父对象）的指针。 Git 的分支，其实本质上仅仅是指向提交对象的可变指针。 Git 的默认分支名字是 master。 在多次提交操作之后，你其实已经有一个指向最后那个提交对象的 master 分支。 它会在每次的提交操作中自动向前移动。 常用命令代码提交和同步代码 git add &lt;file&gt; 将指定文件添加到暂存区，准备提交。 用 git add . 可以将所有修改的文件添加到暂存区。 git commit -m &quot;message&quot; 将暂存区的更改提交到本地仓库，使用 -m 选项添加提交说明。 git status 查看当前分支的状态，包括已修改但未提交的文件。 git log 查看提交历史，使用 --oneline 显示简洁的提交日志。 git push &lt;remote&gt; &lt;branch&gt; 将本地的提交同步到远程仓库的指定分支。 常用 git push origin master 将本地 master 分支推送到远程仓库。 git pull &lt;remote&gt; &lt;branch&gt; 从远程仓库的指定分支拉取最新的更改并合并到当前分支，等价于 fetch + merge 操作。 代码撤销和撤销同步已修改，但未暂存 12345678910111213$ git diff # 列出所有的修改$ git diff xx/xx.py xx/xx2.py # 列出某(几)个文件的修改$ git checkout # 撤销项目下所有的修改$ git checkout . # 撤销当前文件夹下所有的修改$ git checkout xx/xx.py xx/xx2.py # 撤销某几个文件的修改$ git clean -f # untracked状态，撤销新增的文件$ git clean -df # untracked状态，撤销新增的文件和文件夹# Untracked files:# (use &quot;git add &lt;file&gt;...&quot; to include in what will be committed)##\txxx.py 已暂存，未提交 这个时候已经执行过git add，但未执行git commit，但是用git diff已经看不到任何修改。 因为git diff检查的是工作区与暂存区之间的差异。 12345$ git diff --cached # 这个命令显示暂存区和本地仓库的差异$ git reset # 暂存区的修改恢复到工作区$ git reset --soft # 与git reset等价，回到已修改状态，修改的内容仍然在工作区中$ git reset --hard # 回到未修改状态，清空暂存区和工作区 git reset –hard 操作等价于 git reset 和 git checkout 2步操作 已提交，未推送 执行完commit之后，会在仓库中生成一个版本号(hash值)，标志这次提交。之后任何时候，都可以借助这个hash值回退到这次提交。 12345678$ git diff &lt;branch-name1&gt; &lt;branch-name2&gt; # 比较2个分支之间的差异$ git diff master origin/master # 查看本地仓库与本地远程仓库的差异$ git reset --hard origin/master # 回退与本地远程仓库一致$ git reset --hard HEAD^ # 回退到本地仓库上一个版本$ git reset --hard &lt;hash code&gt; # 回退到任意版本$ git reset --soft/git reset # 回退且回到已修改状态，修改仍保留在工作区中。$ git revert &lt;commit&gt; # 创建一个新的提交，用于撤销指定的提交，适合用于已推送到远程的提交记录。 已推送到远程 12$ git push -f orgin master # 强制覆盖远程分支$ git push -f # 如果之前已经用 -u 关联过，则可省略分支名 慎用，一般情况下，本地分支比远程要新，所以可以直接推送到远程，但有时推送到远程后发现有问题，进行了版本回退，旧版本或者分叉版本推送到远程，需要添加 -f参数，表示强制覆盖。 远程操作 git remote -v 查看当前仓库关联的所有远程仓库及其地址。 git remote add &lt;name&gt; &lt;url&gt; 添加新的远程仓库，使用指定的名称（如 origin）。 git fetch &lt;remote&gt; 从远程仓库获取更新但不合并，可以单独检查远程分支的更改。 git push &lt;remote&gt; &lt;branch&gt; 将本地分支推送到远程仓库。 使用 --force 可以强制推送（谨慎使用，以防覆盖他人提交）。 git pull &lt;remote&gt; &lt;branch&gt; 从远程仓库获取并合并更新到当前分支。 git clone &lt;url&gt; 从远程仓库克隆整个项目到本地，创建一个包含远程仓库链接的副本。 记录一些自己在日常使用场景中遇到的问题1. 高效管理工作流：基于 upstream 的 develop 分支创建分支，并将变更推送到 origin : 从主项目的 develop 分支上拉取了最新代码，并且要基于此进行开发。完成修改后，希望将代码推送到自己仓库的 origin 远程仓库，随后提交 PR 以合并到主项目的 develop。 操作步骤: 添加远程仓库：首先确认远程仓库的配置，以确保有 upstream 指向主项目仓库。 12git remote add upstream &lt;主项目仓库地址&gt; # 添加远程项目git fetch upstream # 拉取远程分支更新 从 upstream 更新本地分支： 12git checkout developgit pull upstream develop # 拉取主项目最新的 develop 分支代码 创建新分支： 在本地的 develop 分支上创建新功能分支（如 feature-xyz）。1git checkout -b feature-xyz 提交代码并推送至 origin： 123git add . # 添加修改git commit -m &quot;Add feature XYZ&quot; # 提交修改git push origin feature-xyz # 推送到 origin 远程仓库 创建 PR： 在 GitHub 上打开 PR，将 origin 的 feature-xyz 分支合并至主项目的 develop 分支。 2. 使用 squash 合并提交记录当完成了某个功能的开发，但过程中有多个不必要的中间提交，为保持提交记录的简洁，可以在合并时使用 squash 将多个提交压缩为一个。 多次提交的检查： 使用 git log 查看提交历史，确认需要压缩的提交数量。 交互式 rebase： 使用交互式 rebase 合并提交。1git rebase -i HEAD~n # n 为想 squash 的提交数量 将除了第一个提交外的所有提交前缀改为 squash（或缩写 s）。 修改提交信息： 合并后，Git 会打开编辑器，让你为压缩的提交编写新的提交说明。确保信息完整并清晰。 推送更改： 如果已经将这些提交推送到远程仓库，可能需要使用 --force 强制推送。1git push origin feature-branch --force 3. rebase 和 merge 的使用区别在多人协作开发中，代码整合往往需要处理不同分支的合并。rebase 和 merge 是两种主要的合并方法，但它们的应用场景和效果不同。merge 会保留提交历史，适合较大的历史分支合并；rebase 则用于保持提交记录简洁、线性，更适合功能分支的整合。理解这两者的差异并灵活使用可以避免提交历史的冗杂和冲突的复杂化。 使用 merge 合并场景：保留原始的提交历史，如主分支合并功能分支。 拉取主分支最新代码： 确保 main 上没有新的变更，以避免冲突。12git checkout maingit pull origin main 合并功能分支： 将功能分支（如 feature-xyz）合并至 main。1git merge feature-xyz 推送更改： 推送到远程仓库，并保持功能分支独立的提交记录。1git push origin main 结果：使用 merge 保留了分支开发的提交记录，适合在主分支中查看开发历史。 使用 rebase 合并：场景：让功能分支的提交记录与主分支保持线性历史。适合希望将多个提交压缩并避免产生多余合并节点的场景。 拉取主分支最新代码： 12git checkout maingit pull origin main 切换到功能分支并执行 rebase： 使用 rebase 将 feature-xyz 更新到最新的 main。12git checkout feature-xyzgit rebase main 解决冲突并继续： 如果遇到冲突，按提示解决冲突并继续。12git add &lt;file&gt; # 标记冲突已解决git rebase --continue 推送更改： 推送时需使用 --force 覆盖远程分支。1git push origin feature-xyz --force-with-lease --force-with-lease 是一种更安全的强制推送方式，它会检查远程仓库的分支是否已经被其他人更新过。如果没有被其他人更新过，它才会进行推送，避免无意中覆盖他人的工作。 结果：rebase 将分支历史线性化，保持主分支清晰简洁，适合功能开发的合并。 使用rebase的注意事项： 避免在共享分支上使用：当一个分支已经被推送到远程仓库并且其他人可能基于它进行工作时，不要使用 rebase，因为它会重写历史，导致其他人需要强制同步。 记录一次rebase的翻车事故： 提交的pr被我关闭了几天，这期间仓库有了新的提交。我基于原来提交pr的分支加了新的提交，然后使用rebase，再force push到自己的仓库，然后试着重新打开pr的时候，打不开了 the main branch was force-pushed or recreated","tags":["编程学习","版本管理"]},{"title":"深入理解java泛型","path":"/2024/03/27/深入理解java泛型/","content":"认识java的泛型机制泛型机制（Generics）是一种编程语言特性。允许在编写代码时使用参数化类型。它允许开发者在设计类、接口和方法时使用类型参数，这些类型参数可以在使用时被实际的类型替换。泛型机制的主要目的是增加代码的灵活性、可重用性和类型安全性。 我们通过一个简单的例子认识java的泛型以及感受一下泛型的好处： 假设我们需要实现一个加法功能，支持多种数据类型进行相加。 我们可以使用重载写多个add方法： 1234567891011121314private static int add(int a, int b) &#123; System.out.println(a + &quot;+&quot; + b + &quot;=&quot; + (a + b)); return a + b;&#125;private static float add(float a, float b) &#123; System.out.println(a + &quot;+&quot; + b + &quot;=&quot; + (a + b)); return a + b;&#125;private static double add(double a, double b) &#123; System.out.println(a + &quot;+&quot; + b + &quot;=&quot; + (a + b)); return a + b;&#125; 这样做每种类型都需要重载一个add方法；而通过泛型，我们可以复用为一个方法： 12345//这里定义了一个泛型方法，返回值类型为double,方法接收的参数类型为Number类型及其子类。private static &lt;T extends Number&gt; double add(T a, T b) &#123; System.out.println(a + &quot;+&quot; + b + &quot;=&quot; + (a.doubleValue() + b.doubleValue())); return a.doubleValue() + b.doubleValue();&#125; 再看这个例子，向一个集合dogs中添加3个Dog对象： 123456789101112131415161718192021222324252627class Dog &#123; private String name; Dog()&#123;&#125; Dog(String name) &#123; this.name = name; &#125;&#125;class Cat &#123; private String name; Cat()&#123;&#125; Cat(String name) &#123; this.name = name; &#125;&#125;public class Test&#123; public static void main(String[] args) &#123; //传统写法 List dogs = new ArrayList(); dogs.add(new Dog(&quot;大黄&quot;)); dogs.add(new Dog(&quot;大白&quot;)); dogs.add(new Dog(&quot;小黑&quot;)); //手动类型转换 Dog dog1 = (Dog)dogs.get(0); &#125;&#125; 这样写存在什么问题呢？ 需要手动进行类型转换：因为我们在声明List的时候并没有指定集合当中元素的类型，ArrayList只是维护了一个Object引用的数组。我们接收这个对象就需要进行一次类型转换：Dog dog1 = (Dog)dogs.get(0); 进行强制类型转换效率较低，并且可能会抛出类转换异常ClassCastException。而这个异常我们无法在编译中发现，只能在运行时才能发现，存在安全隐患。 不能对集合中元素的类型进行约束：在我们的需求中是往dogs中添加Dog对象。但是如果我向集合中添加其他类型的元素编译时却不会有任何错误提示：dogs.add(new Cat(&quot;阿猫&quot;)) ，而是在运行时我们取到这个Cat类型的元素并使用（Dog）进行转换时抛出来ClassCastException。 这显然不是我们所期望的，如果程序有潜在的错误，我们更期望在编译时被告知错误，而不是在运行时报异常。 使用泛型解决这个问题： 12345678910public static void main(String[] args) &#123; List&lt;Dog&gt; dogs = new ArrayList(); dogs.add(new Dog(&quot;大黄&quot;)); dogs.add(new Dog(&quot;大白&quot;)); dogs.add(new Dog(&quot;小黑&quot;)); //取元素的时候不需要手动类型转换 Dog dog1 = dogs.get(0); //插入不符合预期的类型会报错 //dogs.add(new Cat(&quot;小猫老弟&quot;)); The method add(Dog) in the type List&lt;Dog&gt; is not applicable for the arguments (Cat)&#125; 在面向对象编程语言中，多态算是一种泛化机制。例如，你可以将方法的参数类型设置为基类，那么该方法就可以接受从这个基类中导出的任何类作为参数，这样的方法将会更具有通用性。此外，如果将方法参数声明为接口，将会更加灵活。 通过这两个例子，我们可以很好理解为什么使用泛型能增强代码的复用性和类型安全性。接下来我们进一步认识java当中泛型的使用方式。 泛型基本使用泛型很好地增强了代码的灵活性和通用性。在泛型使用过程中，操作的数据类型被指定为一个参数，这种参数类型可以用在类、接口和方法中，分别被称为泛型类、泛型接口、泛型方法。 泛型类将泛型使用在类上就称为泛型类。定义泛型类： 1234567891011class Point&lt;T&gt; &#123; // &lt;T&gt; 是泛型标识，代表你定义了一个类型变量 T。T代表的类型由外部决定 // 此处可以随便写标识符号，T是type的简称。 private T val ;\tpublic T getVar()&#123; return var ; &#125; public void setVar(T var)&#123; this.var = var ; &#125; &#125; 使用泛型类：在创建泛型类对象的时候指定具体类型 12345public class Demo&#123; public static void main(String[] args) &#123; Point&lt;String&gt; point = new Point&lt;&gt;();//指定具体类型 &#125;&#125; 创建泛型类对象的时候还可以这样写： Point&lt;String&gt; point = new Point&lt;String&gt;(); 或者 Point point = new Point&lt;String&gt;();，但是一般的写法是在声明引用类型时（左边）指定泛型。具体原因学习完泛型的原理后就能理解。 也可以在类中用不同的泛型标识符指定多个泛型： 12345678910111213141516class MyEntity&lt;K,V&gt; &#123; private K key; private V value; public MyEntity()&#123;&#125; public MyEntity(K key , V val) &#123; this.key = key; this.value = val; &#125; //省略getter和setter&#125;public class TestMap &#123; public static void main(String[] args) &#123; MyEntity&lt;String,Integer&gt; map = new MyEntity&lt;&gt;(&quot;money&quot;,100); &#125;&#125; 泛型接口当将泛型使用在接口上的时候就成为泛型接口，使用方法与泛型类相同。看一个例子： 123456789101112131415161718192021interface Info&lt;T&gt;&#123; // 在接口上定义泛型 public T getVar() ; // 定义抽象方法，抽象方法的返回值就是泛型类型 &#125; class InfoImpl&lt;T&gt; implements Info&lt;T&gt;&#123; // 定义泛型接口的实现类 private T var ; // 定义属性 public InfoImpl(T var)&#123; // 通过构造方法设置属性内容 this.setVar(var) ; &#125; public void setVar(T var)&#123; this.var = var ; &#125; public T getVar()&#123; return this.var ; &#125; &#125; public class GenericsDemo&#123; public static void main(String arsg[])&#123; Info&lt;String&gt; i = new InfoImpl&lt;&gt;(&quot;汤姆&quot;) ; // 通过实现类实例化对象 System.out.println(&quot;内容：&quot; + i.getVar()) ; &#125; &#125; 泛型方法泛型方法则是在调用的时候才指定具体的类型。可以在普通类中定义泛型方法，也可以在泛型类中定义。 语法格式是在方法签名的返回值类型之前指定泛型。举一个栗子： 1234567891011public class Test &#123; public static void main(String[] args) &#123; Test.&lt;String,Integer&gt;f1(&quot;cjp&quot;,666); //指定泛型 TypeEarse.f1(&quot;cjp&quot;,666); //不直接指定泛型 &#125; private static &lt;T,E&gt; void f1(T p1,E p2) &#123; System.out.println(&quot;T&quot; + p2); &#125;&#125; 在使用泛型方法的时候需要注意，在泛型类或者接口中带有泛型标识符方法并不一定是泛型方法。比如： public void eat(E e)&#123;...&#125; 这个方法并不是泛型方法，只是eat方法使用了泛型。 泛型的通配符与上下限在我们阅读java代码的时候会遇到这种泛型： 1default void sort(Comparator&lt;? super E&gt; c) &#123;……&#125; 在这里sort方法接收一个Comparator类型的参数c，而Comparator（比较器）是一个泛型接口，我们给比较器指定了泛型&lt;? super E&gt;,表示Comparator可以接受E以及E的父类。这样做的好处是可以更灵活地使用比较器。这也是java泛型中的一个重要的知识点，泛型的上下限。我们来看一个例子： 12345678910111213141516171819class A&#123;&#125;class B extends A &#123;&#125;// 如下两个方法不会报错public static void funA(A a) &#123; &#125;public static void funB(B b) &#123; funA(b); //父类的引用可以用来接收子类对象 &#125;// 如下funD方法会报错public static void funC(List&lt;A&gt; listA) &#123; // ... &#125;public static void funD(List&lt;B&gt; listB) &#123; funC(listB); // Unresolved compilation problem: The method doPrint(List&lt;A&gt;) in the type test is not applicable for the arguments (List&lt;B&gt;) // ... &#125; 泛型并不具备“继承性”，比如：ArrayList&lt;Object&gt; obj = new ArrayList&lt;String&gt;();❎ ，例子中func接收的List的泛型A,而传入函数的参数是List&lt;B&gt;，虽然A与B有继承关系，但是泛型却不允许这样转换（为什么？可以留着这个问题，了解了泛型的原理之后再来思考）。这也是上个例子报错的原因。 但是程序确实需要这样的需求，比如在一个集合中添加某个类或者该类的实现类（举一个具体的例子，向一个List animal中添加元素，可以是猫，狗，鸟…继承了Animal的类的对象）。直接使用泛型就会存在问题。 为了解决泛型中隐含的转换问题，Java泛型加入了类型参数的上下边界机制。&lt;? extends A&gt;表示该类型参数可以是A(上边界)或者A的子类类型。编译时擦除到类型A，即用A类型代替类型参数。这种方法可以解决开始遇到的问题，编译器知道类型参数的范围，如果传入的实例类型B是在这个范围内的话允许转换，这时只要一次类型转换就可以了，运行时会把对象当做A的实例看待。 1234567public static void funC(List&lt;? extends A&gt; listA) &#123; // ... &#125;public static void funD(List&lt;B&gt; listB) &#123; funC(listB); // OK : funC接收的List的泛型类型是A及其子类 // ... &#125; 在使用泛型的时候，我们可以为传入的泛型类型实参进行上下边界的限制，如：类型实参只准传入某种类型的父类或某种类型的子类。 上限： &lt;T extends Number&gt; 1234567891011121314151617class Info&lt;T extends Number&gt;&#123; // 此处泛型可以是Number或者继承了Number的类，比如Integer,Double... private T var ; public void setVar(T var)&#123; this.var = var ; &#125; public T getVar()&#123; return this.var ; &#125; public String toString()&#123; return this.var.toString() ; &#125;&#125;public class demo1&#123; public static void main(String args[])&#123; Info&lt;Integer&gt; i1 = new Info&lt;Integer&gt;() ; // 声明Integer的泛型对象 &#125;&#125; 下限： &lt;? super String&gt; 12345678910111213141516171819202122232425class Info&lt;T&gt;&#123; private T var ; public void setVar(T var)&#123; this.var = var ; &#125; public T getVar()&#123; return this.var ; &#125; public String toString()&#123; return this.var.toString() ; &#125;&#125;public class GenericsDemo21&#123; public static void main(String args[])&#123; Info&lt;String&gt; i1 = new Info&lt;String&gt;() ; // 声明String的泛型对象 Info&lt;Object&gt; i2 = new Info&lt;Object&gt;() ; // 声明Object的泛型对象 i1.setVar(&quot;hello&quot;) ; i2.setVar(new Object()) ; fun(i1) ; fun(i2) ; &#125; public static void fun(Info&lt;? super String&gt; temp)&#123; // 只能接收String或Object类型的泛型，String类的父类只有Object类 System.out.print(temp + &quot;, &quot;) ; &#125;&#125; 如果对类型的上界或者下界有多个限制，可以使用 &amp; : 1234567891011public class Client &#123; //工资低于2500元的上斑族并且站立的乘客车票打8折 public static &lt;T extends Staff &amp; Passenger&gt; void discount(T t)&#123; if(t.getSalary()&lt;2500 &amp;&amp; t.isStanding())&#123; System.out.println(&quot;恭喜你！您的车票打八折！&quot;); &#125; &#125; public static void main(String[] args) &#123; discount(new Me()); &#125;&#125; 至此我们已经基本了解了在java中使用泛型的基本语法。接下来我们继续探究java泛型的实现原理以及使用的细节。 泛型的原理与使用细节java的伪泛型 ：类型擦除java的泛型策略实际上是一种伪泛型。即在语法上支持泛型，但在编译阶段会将所有的泛型（尖括号括起来的内容）都还原成原始类型（Row Type）。比如List&lt;Integer&gt; list 在编译之后就变成：List list。这也是类型擦除（type erasure）的含义。 为什么要使用类型擦除来实现泛型呢？这样实现有什么好处？ 泛型机制是在jdk5引入的，为了兼容以前的版本，才采取了这种策略。这样使用新版的jdk写的程序编译之后与以前的代码是兼容的，就不需要重构旧的代码。 减轻 JVM 的负担，提高运行期的效率。如果 JVM 将泛型类型延续到运行期，那么到运行期时 JVM 就需要进行大量的重构工作。 接下来我们来看几个具体的类型擦除的例子： 假设我们重载了这两个方法，编译器却报错： Erasure of method f1(List&lt;String&gt;) is the same as another method in type TypeEarse 即类型擦除后两个函数的签名是一致的，形参都变成 : List p1这是两个一样的函数。 1234567public class Test &#123; private void f1(List&lt;String&gt; p1) &#123; &#125; private void f1(List&lt;Integer&gt; p2) &#123; &#125;&#125; 再考虑一下下面的程序： 12345678910111213141516public class Test &#123; public static void main(String[] args) throws Exception &#123; ArrayList&lt;Integer&gt; list = new ArrayList&lt;Integer&gt;(); list.add(1); //这样调用 add 方法只能存储整形，因为泛型类型的实例为 Integer list.getClass().getMethod(&quot;add&quot;, Object.class).invoke(list, &quot;asd&quot;); for (int i = 0; i &lt; list.size(); i++) &#123; System.out.println(list.get(i)); &#125; &#125;&#125; 程序中声明了一个存储Integer的ArrayList，直接调用add()方法只能存储整型变量。添加其他类型的元素： add(&quot;cjp&quot;)会报错 : The method add(Integer) in the type ArrayList&lt;Integer&gt; is not applicable for the arguments (String)。而我们如果使用反射就可以在运行过程中向集合中添加字符型类型的元素。这表明了在编译期间，ArrayList&lt;Integer&gt; list的类型是被擦除了的，还原成了原始类型：ArrayList，里面维护的Object数组，我们可以正常添加任何类型的元素。 那么类型擦除是如何进行的？ 类型擦除的原则： 消除类型参数声明，即删除&lt;&gt;及其包围的部分。 根据类型参数的上下界推断并替换所有的类型参数为原生态类型：如果类型参数是无限制通配符或没有上下界限定则替换为Object，如果存在上下界限定则根据子类替换原则取类型参数的最左边限定类型（即父类）。 为了保证类型安全，必要时插入强制类型转换代码。 自动产生“桥接方法”以保证擦除类型后的代码仍然具有泛型的“多态性”。 例子： 擦除类定义中的类型参数 - 无限制类型擦除 当类定义中的类型参数没有任何限制时，在类型擦除中直接被替换为Object，即形如&lt;T&gt;和&lt;?&gt;的类型参数都被替换为Object。 擦除类定义中的类型参数 - 有限制类型擦除 当类定义中的类型参数存在限制（上下界）时，在类型擦除中替换为类型参数的上界或者下界，比如形如&lt;T extends Number&gt;和&lt;? extends Number&gt;的类型参数被替换为Number，&lt;? super Number&gt;被替换为Object。（Number的父类为Object） 擦除方法定义中的类型参数 擦除方法定义中的类型参数原则和擦除类定义中的类型参数是一样的，这里仅以擦除方法定义中的有限制类型参数为例。 泛型的编译期检查我们前面已经知道了泛型会在编译阶段被擦除成原生类型。那么为什么当我们往List&lt;Integer&gt; list这个集合中添加其它类型的元素的时候编译器会报错呢？编译期间不是都变成了Object了吗，我添加String为什么还会报错？ 123456public static void main(String[] args) &#123; List&lt;Integer&gt; list = new ArrayList&lt;&gt;(); list.add(123); list.add(&quot;123&quot;); //编译错误 &#125; 因为编译器会在编译之前先进行对泛型类型的检查，再进行类型擦除。那么这个检查是如何进行的呢？这个类型检查是针对谁的呢？我们先看看参数化类型和原始类型的兼容。 以 ArrayList举例子，以前的写法: 1ArrayList list = new ArrayList(); 现在的写法: 1ArrayList&lt;String&gt; list = new ArrayList&lt;String&gt;(); 如果是与以前的代码兼容，各种引用传值之间，必然会出现如下的情况： 12ArrayList&lt;String&gt; list1 = new ArrayList(); //第一种 情况ArrayList list2 = new ArrayList&lt;String&gt;(); //第二种 情况 这样是没有错误的，不过会有个编译时警 : ArrayList is a raw type. References to generic type ArrayList&lt;E&gt; should be parameterized。在第一种情况，可以实现与完全使用泛型参数一样的效果，第二种则没有效果。 因为类型检查就是编译时完成的，new ArrayList()只是在内存中开辟了一个存储空间，可以存储任何类型对象，而真正涉及类型检查的是它的引用，因为我们是使用它引用list1来调用它的方法，比如说调用add方法，所以list1引用能完成泛型类型的检查。而引用list2没有使用泛型，所以不行。 举例子： 1234567891011121314151617181920public class Test &#123; public static void main(String[] args) &#123; ArrayList&lt;String&gt; list1 = new ArrayList(); list1.add(&quot;1&quot;); //编译通过 list1.add(1); //编译错误 String str1 = list1.get(0); //返回类型就是String ArrayList list2 = new ArrayList&lt;String&gt;(); list2.add(&quot;1&quot;); //编译通过 list2.add(1); //编译通过 Object object = list2.get(0); //返回类型就是Object new ArrayList&lt;String&gt;().add(&quot;11&quot;); //编译通过 new ArrayList&lt;String&gt;().add(22); //编译错误 String str2 = new ArrayList&lt;String&gt;().get(0); //返回类型就是String &#125; &#125; 通过上面的例子，我们可以明白，类型检查就是针对引用的，谁是一个引用，用这个引用调用泛型方法，就会对这个引用调用的方法进行类型检测，而无关它真正引用的对象。 泛型的多态与桥接方法类型擦除会造成多态的冲突。例子： 12345678910111213141516171819202122class Holder&lt;T&gt; &#123; private T value; public T getValue() &#123; return value; &#125; public void setValue(T value) &#123;&#125; &#125;class DateHolder extends Holder&lt;Date&gt; &#123; private Date value; @Override public Date getValue() &#123; return this.value; &#125; @Override public void setValue(Date value) &#123; this.value = value; &#125;&#125; 在这里我们定义了一个泛型接口，定义一个子类实现泛型接口，重写了接口的两个方法。我们的原意是这样的：将父类的泛型类型限定为Date，那么父类里面的两个方法的泛型参数&lt;T&gt;都为Date类型。所以，我们在子类中重写这两个方法一点问题也没有，实际上，从他们的@Override标签中也可以看到，一点问题也没有，实际上是这样的吗？ 分析：实际上，类型擦除后，父类的的泛型类型全部变为了原始类型Object，所以父类接口编译之后会变成下面的样子： 12345678910class Holder &#123; public Object getValue() &#123; //... &#125; public void setValue(Object obj) &#123; //... &#125;&#125; 而实现类重写的方法： 1234567public Date getValue() &#123; return this.value; &#125; public void setValue(Date value) &#123; this.value = value; &#125; 在setValue方法中，父类方法与子类的形参的类型不同，这不是重写，而是重载。假如是重载，子类会有两个重载的setValue方法，我们测试一下： 1234567 public class TestPolymorphic &#123; public static void main(String[] args) &#123; DateHolder dateHolder = new DateHolder(); dateHolder.setValue(new Date()); dateHolder.setValue(new Object()); //编译出错：The method setValue(Date) in the type DateHolder is not applicable for the arguments (Object) &#125;&#125; 说明实现类中并没有形参为？Object的setValue()方法。这并不是重载，而是真的重写了父类的方法。由于种种原因，虚拟机并不能将泛型类型变为Date，只能将类型擦除掉，变为原始类型Object。这样，我们的本意是进行重写，实现多态。可是类型擦除后，只能变为了重载。这样，类型擦除就和多态有了冲突。 于是JVM采用了一个特殊的方法，来解决泛型中多态的冲突，那就是桥接方法。 我们使用javap -c 对DateHolder进行反汇编： 12345678910111213141516171819202122232425262728293031323334353637javap -c DateHolder Compiled from &quot;DateHolder.java&quot;public class test4blog.DateHolder extends test4blog.Holder&lt;java.util.Date&gt; &#123; public test4blog.DateHolder(); Code: 0: aload_0 1: invokespecial #1 // Method test4blog/Holder.&quot;&lt;init&gt;&quot;:()V 4: return public java.util.Date getValue(); Code: 0: aload_0 1: getfield #2 // Field value:Ljava/util/Date; 4: areturn public void setValue(java.util.Date); Code: 0: aload_0 1: aload_1 2: putfield #2 // Field value:Ljava/util/Date; 5: return public void setValue(java.lang.Object); //编译时由编译器生成的桥方法 Code: 0: aload_0 1: aload_1 2: checkcast #3 // class java/util/Date 5: invokevirtual #4 // Method setValue:(Ljava/util/Date;)V 8: return public java.lang.Object getValue(); //编译时由编译器生成的桥方法 Code: 0: aload_0 1: invokevirtual #5 // Method getValue:()Ljava/util/Date; 4: areturn&#125; 从反编译的结果看，子类有4方法，其中最后的两个方法，就是编译器自己生成的桥方法。可以看到桥方法的参数类型都是Object，也就是说，子类中真正覆盖父类两个方法的就是这两个我们看不到的桥方法。而打在我们自己定义的setvalue和getValue方法上面的@Override只不过是假象。而桥方法的内部实现，就只是去调用我们自己重写的那两个方法。 所以，虚拟机巧妙的使用了桥方法，来解决了类型擦除和多态的冲突。 并且，还有一点也许会有疑问，子类中的桥方法Object getValue()和Date getValue()是同时存在的，可是如果是常规的两个方法，他们的方法签名是一样的，也就是说虚拟机根本不能分别这两个方法。如果是我们自己编写Java代码，这样的代码是无法通过编译器的检查的，但是虚拟机却是允许这样做的，因为虚拟机通过参数类型和返回类型来确定一个方法，所以编译器为了实现泛型的多态允许自己做这个看起来“不合法”的事情，然后交给虚拟器去区别。 泛型疑难杂症Ｑ＆A 如何理解类型擦除之后的原始类型？ 原始类型 就是擦除去了泛型信息，最后在字节码中的类型变量的真正类型，无论何时定义一个泛型，相应的原始类型都会被自动提供，类型变量擦除，并使用其限定类型（无限定的变量用Object）替换。类型擦除之后代码是与没有泛型机制之前一样的（即使用Object类型来接收其它任意类型的变量）。 以List&lt;String&gt;，List&lt;Integer&gt; 为例，它们的原始类型都是List。在你声明泛型的时候，就可以根据类型类型擦除的原则确定该泛型的原始类型。 在声明泛型类的对象或者调用泛型方法时，不指定泛型编译也可以通过，这时候的对象中的相关成员类型是什么？ 注意区分一下泛型变量的类型和原始类型的概念。泛型变量的类型是指编译（类型擦除）之前，进行语法检查时使用的类型。原始类型是指泛型变量擦除去了泛型信息，最后在字节码中的类型变量的真正类型。 在泛型类或者泛型接口中，使用泛型类或者接口创建对象时不指定泛型，默认泛型类型为Object。 比如 12345public static void main(String[] args) &#123; ArrayList arrayList = new ArrayList(); //创建ArrayList不指定泛型，则默认类型为Object arrayList.add(1); //可以存储任何类型的变量 arrayList.add(&quot;cjp&quot;);&#125; 在调用泛型方法时，可以指定泛型，也可以不指定泛型: 在不指定泛型的情况下，泛型变量的类型为该方法中的几种类型的同一父类的最小级，直到Object 在指定泛型的情况下，该方法的几种类型必须是该泛型的实例的类型或者其子类 12345678910111213141516171819public class Test &#123; public static void main(String[] args) &#123; /**不指定泛型的时候*/ int i = Test.add(1, 2); //这两个参数都是Integer，所以T为Integer类型 Number f = Test.add(1, 1.2); //这两个参数一个是Integer，一个是Float，所以取同一父类的最小级，为Number Object o = Test.add(1, &quot;asd&quot;); //这两个参数一个是Integer，一个是String，所以取同一父类的最小级，为Object /**指定泛型的时候*/ int a = Test.&lt;Integer&gt;add(1, 2); //指定了Integer，所以只能为Integer类型或者其子类 int b = Test.&lt;Integer&gt;add(1, 2.2); //编译错误，指定了Integer，不能为Float Number c = Test.&lt;Number&gt;add(1, 2.2); //指定为Number，所以可以为Integer和Float &#125; //这是一个简单的泛型方法 public static &lt;T&gt; T add(T x,T y)&#123; return y; &#125; &#125; 泛型为什么没有继承的传递关系？ 就比如前面举的例子 ArrayList&lt;Object&gt; obj = new ArrayList&lt;String&gt;();❎ 在Java中，泛型是不支持协变的。这意味着即使 ArrayList&lt;String&gt; 是 ArrayList&lt;Object&gt; 的子类型，ArrayList&lt;String&gt; 也不能赋值给 ArrayList&lt;Object&gt;。我们假设java允许这么做,再来看看这样会存在哪些问题。 &#x2F;&#x2F;TODO : 待理解 来自GPT的回答： 如果 Java 允许 ArrayList&lt;String&gt; 赋值给 ArrayList&lt;Object&gt;，那么可能会导致以下问题： 类型安全性问题： Java 的泛型是在编译时进行类型检查的，而不是在运行时。如果允许将 ArrayList&lt;String&gt; 赋值给 ArrayList&lt;Object&gt;，则编译器无法在编译时捕获到潜在的类型错误。例如，当您尝试将一个 Integer 对象添加到 ArrayList&lt;String&gt; 中时，编译器不会报错，但在运行时会抛出 ClassCastException。 破坏集合的封装性： 泛型的目的之一是增强代码的可读性和安全性，通过明确指定集合中元素的类型，可以更清晰地了解代码的意图。如果允许将 ArrayList&lt;String&gt; 赋值给 ArrayList&lt;Object&gt;，则破坏了集合的封装性，可能会导致代码更加难以理解和维护。 破坏泛型的不变性： 泛型具有不变性，即泛型类型参数在赋值后不能被修改。如果允许将 ArrayList&lt;String&gt; 赋值给 ArrayList&lt;Object&gt;，则破坏了泛型的不变性，可能导致在编译时无法检测到潜在的类型不一致问题，从而引入安全隐患。 综上所述，虽然看起来允许 ArrayList&lt;String&gt; 赋值给 ArrayList&lt;Object&gt; 可以提供一定的便利性，但这种做法会破坏泛型的安全性和封装性，可能引入类型错误和安全隐患。因此，Java 不允许这种类型的赋值。 为什么泛型类型不能实例化？ T obj = new T()❎ 因为在 Java 编译期没法确定泛型参数化类型，也就找不到对应的类字节码文件，所以自然就不行了，此外由于T 被擦除为 Object，如果可以 new T() 则就变成了 new Object()，失去了本意。 如果我们确实需要实例化一个泛型，应该如何做呢？可以通过反射实现： 1234static &lt;T&gt; T newTclass (Class &lt; T &gt; clazz) throws InstantiationException, IllegalAccessException &#123; T obj = clazz.newInstance(); return obj;&#125; List&lt;?&gt;、List&lt;Object&gt;、List&lt;? extends Object&gt;有什么区别？ 泛型类型不同，List&lt;?&gt;使用&lt;?&gt;通配符,接收任意类型;List&lt;? extends Object&gt;的泛型参数允许接收Object及其子类。 为什么泛型不能支持基本数据类型？ 就像这个例子： List&lt;Integer&gt; list = new ArrayList&lt;&gt;() ✅ List&lt;int&gt; list = new ArrayList&lt;&gt;() ❎ 因为对泛型进行类型擦除之后成员类型是Object，而Object无法接收基本数据类型的变量：int a = 10; Object obj = a;❎ 声明和使用泛型数组存在哪些问题？ 不能使用new创建泛型数组：private T[] array = new T[10];❎Cannot create a generic array of T 。使用new创建数组是在内存开辟一块指定大小的内存，使用泛型无法确定开辟的内存大小。 类型擦除后类型为Object[ ] , 为什么不直接规定这样创建就是开辟一个指定大小的Object数组呢？ 这样规定就失去了泛型的意义。使用泛型是为了限定类型，如果规定private T[] array = new T[10]在编译之前也等价于private Object[] array = new Object[10]，那么还是不能限定数组内的元素预期的类型，取元素的时候要进行类型转换。直接规定不允许这样使用才合理。 看一下的例子： 123456List&lt;String&gt;[] list11 = new ArrayList&lt;String&gt;[10]; //编译错误 Cannot create a generic array of ArrayList&lt;String&gt;，非法创建 List&lt;String&gt;[] list12 = new ArrayList&lt;?&gt;[10]; //编译错误 Type mismatch: cannot convert from ArrayList&lt;?&gt;[] to List&lt;String&gt;[]，需要强转类型 List&lt;String&gt;[] list13 = (List&lt;String&gt;[]) new ArrayList&lt;?&gt;[10]; //OK，但是会有警告 Type safety: Unchecked cast from ArrayList&lt;?&gt;[] to List&lt;String&gt;[] List&lt;?&gt;[] list14 = new ArrayList&lt;String&gt;[10]; //编译错误 Cannot create a generic array of ArrayList&lt;String&gt;，非法创建 List&lt;?&gt;[] list15 = new ArrayList&lt;?&gt;[10]; //OK List&lt;String&gt;[] list6 = new ArrayList[10]; //OK，但是会有警告 Type safety: The expression of type ArrayList[] needs unchecked conversion to conform to List&lt;String&gt;[] &#x2F;&#x2F;TODO : 理解为什么 静态方法中使用泛型会有什么问题？ 泛型类中的静态方法和静态变量不可以使用泛型类所声明的泛型类型参数。 12345678910111213141516public class UseGenericsInStatic&lt;T&gt; &#123; private T value; private static T staticValuie; //报错：Cannot make a static reference to the non-static type T //静态泛型方法 private static&lt;E&gt;Double test(E e) &#123; System.out.println(&quot;test , para: &quot; + e); return 66.6; &#125; private static Double test1(T t) &#123; //报错：Cannot make a static reference to the non-static type T System.out.println(&quot;test1 , para: &quot; + t); &#125;&#125; 泛型类在创建对象的时候指定泛型的类型，而静态成员和静态方法在类加载（对象创建之前）的时候就加载完。在静态方法或者静态变量中使用泛型无法确定这个泛型参数是何种类型。（还是不要那样想：为啥擦除后是Object，不直接规定Object。这样就没必要存在泛型了！） 参考文章： https://pdai.tech/md/java/basic/java-basic-x-generic.html https://www.cnblogs.com/54chensongxia/p/12470672.html https://stackoverflow.com/questions/36347/what-are-the-differences-between-generic-types-in-c-and-java?spm=a2c6h.12873639.article-detail.10.460a1026WugJsK https://www.cnblogs.com/strongmore/p/13945540.html","tags":["java-se"],"categories":["编程学习","java"]},{"title":"Hello World","path":"/2024/03/25/hello-world/","content":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment"},{"title":"about","path":"/about/index.html","content":"Welcome to my bolg!"}]